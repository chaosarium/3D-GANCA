{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANCA Experimentats & Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import data_helper\n",
    "import importlib\n",
    "importlib.reload(data_helper)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "BLOCK2VEC_OUT_PATH = 'output/block2vec saves/block2vec 64 dim/'\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANCA3DDataset(Dataset):\n",
    "    def __init__(self, loaded_worlds):\n",
    "        super().__init__()\n",
    "        self.dataset = loaded_worlds[:,:,:,:,0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.dataset[index]\n",
    "        return torch.tensor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        # super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.dims = (32, 32, 32) # this will be returned when calling this.size()\n",
    "                \n",
    "    def prepare_data(self):\n",
    "        # download data\n",
    "        data_helper.download_dataset()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # splitting data and process stuff\n",
    "        print('settin gup')\n",
    "        full_dataset = GANCA3DDataset(data_helper.houses_dataset())\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(full_dataset, [1600, 192, 185])\n",
    "        \n",
    "    # these funcs can also be placed directly inside a LightningModule\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # return DataLoader(self.val_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # return DataLoader(self.test_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pass\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BLOCK2VEC_OUT_PATH + \"representations.pkl\", 'rb') as f:\n",
    "\tembeddings_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelNCA(nn.Module):\n",
    "    def __init__(self, \n",
    "            world_shape=(32,32,32), \n",
    "            hidden_size=128,\n",
    "        ):\n",
    "        super(VoxelNCA, self).__init__()\n",
    "        \n",
    "        self.world_shape = world_shape\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input_state):\n",
    "        # in comes a world also with shape (hidden_size, world_shape[0], world_shape[1], world_shape[2])\n",
    "\n",
    "        \n",
    "\n",
    "        return input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 3D artifacts paper experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintShape(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrintShape, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Do your print / debug stuff here\n",
    "        print(\"-> shape is now\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential CNN??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Perception Net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelPerceptionNet(nn.Module):\n",
    "    # Essentially running a trainable perceptor on each layer. This expands the number of channels by a factor of num_perceptions and gives us the visual features to do NCA updates\n",
    "    \n",
    "    def __init__(\n",
    "        self, num_in_channels, num_perceptions=3, normal_std=0.02, use_normal_init=True, zero_bias=True\n",
    "    ):\n",
    "        super(VoxelPerceptionNet, self).__init__()\n",
    "        \n",
    "        self.num_in_channels = num_in_channels\n",
    "        self.normal_std = normal_std\n",
    "        \n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                self.num_in_channels, # incoming channels\n",
    "                self.num_in_channels * num_perceptions, # expand num_in_channels by factor of 3\n",
    "                3, # kernal size of 3, which means neighbour radius of 1\n",
    "                stride=1, # stride of 1 so look at each voxel\n",
    "                padding=1, # make sure to look at edge voxels\n",
    "                groups=self.num_in_channels, # disconnect the perceptions so that multiple percpetions run on each channel; essentially we have num_perceptions convolutional layers side by side\n",
    "                bias=False, # no bias\n",
    "            ),\n",
    "            PrintShape()\n",
    "        )\n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # init weights for Conv3d layers\n",
    "                nn.init.normal_(m.weight, std=normal_std)\n",
    "                \n",
    "                # if bias exist, init bias\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    if zero_bias:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                    else:\n",
    "                        nn.init.normal_(m.bias, std=normal_std)\n",
    "\n",
    "        # weight initialisation\n",
    "        if use_normal_init:\n",
    "            with torch.no_grad():\n",
    "                self.apply(init_weights)\n",
    "                \n",
    "    def forward(self, input):\n",
    "        return self.sequence(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = VoxelPerceptionNet(3, normal_std=0.02, num_perceptions=3, use_normal_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence.0.weight tensor([[[[[-3.8883e-02, -5.1184e-03, -6.4090e-03],\n",
      "           [-5.3794e-02,  1.2653e-02,  6.2345e-03],\n",
      "           [-1.6375e-02,  9.6170e-03,  1.3706e-02]],\n",
      "\n",
      "          [[ 3.4142e-03,  1.6779e-02, -2.0713e-02],\n",
      "           [ 3.3475e-02, -1.6946e-02, -5.9912e-03],\n",
      "           [-2.3480e-03,  4.5203e-02, -3.0542e-02]],\n",
      "\n",
      "          [[-1.0536e-02,  7.1457e-04, -3.8952e-03],\n",
      "           [-5.8230e-03,  1.2197e-02, -2.4776e-03],\n",
      "           [-2.2198e-02, -2.3461e-02, -1.5070e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.5103e-02,  3.6363e-03, -6.9560e-03],\n",
      "           [-2.0942e-02,  7.4115e-04, -2.8962e-02],\n",
      "           [ 3.3001e-02, -4.7898e-03, -5.9040e-03]],\n",
      "\n",
      "          [[ 2.4569e-02, -5.7138e-03, -1.2699e-02],\n",
      "           [ 1.1442e-02,  7.4514e-03, -2.3042e-02],\n",
      "           [-1.8838e-02,  2.4893e-02,  1.4786e-02]],\n",
      "\n",
      "          [[ 2.9716e-02,  1.8288e-02,  6.1580e-03],\n",
      "           [ 1.3902e-02,  1.0811e-02, -3.5035e-02],\n",
      "           [-3.6581e-02,  1.1993e-02, -3.1509e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 4.0772e-03, -2.9125e-03, -5.3883e-02],\n",
      "           [ 1.4236e-02,  1.7160e-02, -6.9886e-03],\n",
      "           [-9.9890e-04,  3.5889e-02,  1.2969e-03]],\n",
      "\n",
      "          [[ 1.9137e-02, -1.2862e-02, -1.3315e-02],\n",
      "           [ 8.9111e-03, -2.5996e-02,  6.4721e-02],\n",
      "           [-8.8489e-03,  1.4547e-02, -2.7608e-02]],\n",
      "\n",
      "          [[-3.0040e-02, -1.1227e-02,  1.0556e-02],\n",
      "           [ 9.0369e-03,  2.0564e-02,  3.8876e-02],\n",
      "           [ 2.3693e-02,  1.4490e-02, -2.7393e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-6.0609e-03, -1.0107e-02, -9.1867e-03],\n",
      "           [ 4.8029e-02,  2.2031e-02,  2.8428e-02],\n",
      "           [ 9.7552e-03, -2.9981e-02, -5.5739e-03]],\n",
      "\n",
      "          [[ 1.7096e-02,  3.3043e-02,  2.5553e-02],\n",
      "           [ 8.2013e-03, -2.8567e-03, -9.5447e-03],\n",
      "           [-8.1262e-03, -3.2493e-03,  2.4063e-02]],\n",
      "\n",
      "          [[ 1.3000e-02, -5.2853e-03, -7.9018e-03],\n",
      "           [-3.2557e-04,  1.1386e-02, -1.8602e-02],\n",
      "           [-2.2846e-03,  2.6551e-02,  4.3995e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 5.0639e-03, -1.4545e-02, -1.9633e-02],\n",
      "           [ 2.8998e-03, -4.8309e-03,  1.2238e-02],\n",
      "           [ 1.9211e-02, -2.5873e-02,  2.3957e-02]],\n",
      "\n",
      "          [[ 6.8265e-02, -2.6297e-02, -1.1625e-03],\n",
      "           [ 5.2456e-03,  1.1832e-02,  9.7704e-03],\n",
      "           [ 1.8281e-02, -3.2211e-05, -2.6138e-03]],\n",
      "\n",
      "          [[-1.2005e-02, -2.4509e-02, -1.8491e-02],\n",
      "           [ 1.0065e-02,  1.8461e-03,  3.5448e-02],\n",
      "           [-2.9632e-02,  2.0531e-02, -8.5045e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.5121e-02, -2.3043e-02,  7.1557e-03],\n",
      "           [-9.6671e-03,  1.6054e-02,  2.7067e-02],\n",
      "           [ 3.3374e-02, -1.2637e-02,  2.1268e-02]],\n",
      "\n",
      "          [[-1.3351e-02,  5.9627e-02, -3.6287e-02],\n",
      "           [-3.0795e-02, -9.6192e-03, -2.4245e-02],\n",
      "           [ 2.2295e-02,  5.7985e-03, -1.4436e-02]],\n",
      "\n",
      "          [[-1.7881e-02,  1.7942e-02, -8.2268e-03],\n",
      "           [-3.9674e-03, -5.1462e-04,  1.1557e-03],\n",
      "           [-8.5829e-03, -3.3504e-03,  1.8054e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.8680e-02,  4.8617e-03, -2.6251e-02],\n",
      "           [ 3.5625e-02,  1.2349e-03, -6.3870e-04],\n",
      "           [-1.4160e-02, -1.9923e-02,  9.4917e-03]],\n",
      "\n",
      "          [[ 2.6622e-02, -7.9676e-03, -1.3991e-02],\n",
      "           [ 5.3957e-02,  6.7246e-03,  7.1338e-03],\n",
      "           [ 2.0790e-02,  3.0724e-02, -2.9296e-04]],\n",
      "\n",
      "          [[-6.6037e-03,  6.6098e-03, -5.4144e-02],\n",
      "           [-3.2190e-02,  3.0602e-02, -9.6887e-03],\n",
      "           [-1.0297e-02,  2.4072e-02,  1.2546e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9479e-02,  1.7864e-02,  3.6054e-03],\n",
      "           [-7.1378e-03,  3.0222e-02,  2.2939e-02],\n",
      "           [ 1.6225e-02,  2.2059e-02, -1.0488e-02]],\n",
      "\n",
      "          [[-1.6139e-02,  1.1788e-02, -1.4075e-02],\n",
      "           [-1.1010e-02, -2.7067e-03,  1.9559e-02],\n",
      "           [-1.3036e-02,  2.9653e-02,  3.4580e-02]],\n",
      "\n",
      "          [[-1.6389e-02,  1.0817e-02, -1.1142e-02],\n",
      "           [ 3.2755e-02,  3.2371e-02, -1.0204e-03],\n",
      "           [ 1.0827e-02,  3.6592e-02, -1.0805e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9658e-02, -7.3658e-03, -8.3242e-04],\n",
      "           [ 1.7139e-02,  1.2000e-02,  2.2171e-02],\n",
      "           [ 8.8215e-04,  3.0328e-02, -2.6914e-02]],\n",
      "\n",
      "          [[ 2.0298e-02,  5.3409e-02,  3.1592e-02],\n",
      "           [ 2.7437e-02, -1.9974e-02, -9.6326e-03],\n",
      "           [ 7.9410e-03,  3.6938e-02,  9.7868e-03]],\n",
      "\n",
      "          [[-2.9307e-02,  1.0572e-03,  6.0766e-03],\n",
      "           [ 1.4563e-02, -2.1380e-02,  2.6580e-02],\n",
      "           [-2.6830e-02, -1.2042e-02,  1.1713e-02]]]]])\n"
     ]
    }
   ],
   "source": [
    "# print all params for debug'\n",
    "for name, param in test2.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> shape is now torch.Size([16, 9, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "out = test2(torch.rand(16, 3, 8, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Update Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelUpdateNet(nn.Module):\n",
    "    # This is essentially running dense nets in parallel for each voxel. It takes visual features and predict the update for each feature. In comes the output from a VoxelPerceptionNet, which is of the shape (N, num_in_channels * num_perceptions, channel_dims[0], channel_dims[1], channel_dims[2])\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels = 16,\n",
    "        num_perceptions=3,\n",
    "        channel_dims=[64, 64],\n",
    "        normal_std=0.02,\n",
    "        use_normal_init=True,\n",
    "        zero_bias=True,\n",
    "    ):\n",
    "        super(VoxelUpdateNet, self).__init__()\n",
    "        \n",
    "        def make_sequental(num_channels, channel_dims):\n",
    "                \n",
    "            # make first layer. \n",
    "            sequence = [\n",
    "                # visual_feature_channels, x, y, z -> channel_dims[0], x, y, z\n",
    "                nn.Conv3d(num_channels * num_perceptions, channel_dims[0], kernel_size=1), \n",
    "                nn.ReLU(), \n",
    "                PrintShape()\n",
    "            ]\n",
    "            \n",
    "            # loop through dims[1:] and make Conv3d\n",
    "            for i in range(1, len(channel_dims)):\n",
    "                sequence.extend([\n",
    "                    nn.Conv3d(channel_dims[i - 1], channel_dims[i], kernel_size=1), \n",
    "                    nn.ReLU(), \n",
    "                    PrintShape()\n",
    "                ])\n",
    "                \n",
    "            # make final layer\n",
    "            sequence.extend([\n",
    "                    nn.Conv3d(channel_dims[-1], num_channels, kernel_size=1, bias=False),\n",
    "                    PrintShape()\n",
    "                ])\n",
    "                        \n",
    "            return nn.Sequential(*sequence)\n",
    "        \n",
    "        self.update_net = make_sequental(num_channels, channel_dims)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # init weights for Conv3d layers\n",
    "                nn.init.normal_(m.weight, std=normal_std)\n",
    "                \n",
    "                # if bias exist, init bias\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    if zero_bias:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                    else:\n",
    "                        nn.init.normal_(m.bias, std=normal_std)\n",
    "\n",
    "        # weight initialisation\n",
    "        if use_normal_init:\n",
    "            with torch.no_grad():\n",
    "                self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.update_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = VoxelUpdateNet(num_channels = 16, num_perceptions=3, channel_dims=[64, 32], normal_std=0.02, use_normal_init=True, zero_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> shape is now torch.Size([3, 64, 8, 8, 8])\n",
      "-> shape is now torch.Size([3, 32, 8, 8, 8])\n",
      "-> shape is now torch.Size([3, 16, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "out = test3(torch.rand(3,48,8,8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VoxelNCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8d7ae5c7f0d87e5344f9e6608bfcb3eb27eaba4a1efb346157a5f4c9c93fcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
