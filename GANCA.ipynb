{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANCA Experimentats & Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import data_helper\n",
    "import importlib\n",
    "importlib.reload(data_helper)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torchsummaryX import summary\n",
    "import os\n",
    "from einops import rearrange\n",
    "import torch.nn as nn\n",
    "\n",
    "BLOCK2VEC_OUT_PATH = 'output/block2vec saves/block2vec 64 dim/'\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANCA3DDataset(Dataset):\n",
    "    def __init__(self, loaded_worlds):\n",
    "        super().__init__()\n",
    "        self.dataset = loaded_worlds[:,:,:,:,0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.dataset[index]\n",
    "        return torch.tensor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset template\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        # super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.dims = (32, 32, 32) # this will be returned when calling this.size()\n",
    "                \n",
    "    def prepare_data(self):\n",
    "        # download data\n",
    "        data_helper.download_dataset()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # splitting data and process stuff\n",
    "        print('settin gup')\n",
    "        full_dataset = GANCA3DDataset(data_helper.houses_dataset())\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = torch.utils.data.random_split(full_dataset, [1600, 192, 185])\n",
    "        \n",
    "    # these funcs can also be placed directly inside a LightningModule\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # return DataLoader(self.val_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # return DataLoader(self.test_dataset, batch_size=self.batch_size, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "        pass\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BLOCK2VEC_OUT_PATH + \"representations.pkl\", 'rb') as f:\n",
    "\tembeddings_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 3D artifacts paper experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Perception Net!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelPerceptionNet(nn.Module):\n",
    "    # Essentially running a trainable perceptor on each layer. This expands the number of channels by a factor of num_perceptions and gives us the visual features to do NCA updates\n",
    "    \n",
    "    def __init__(self, \n",
    "        num_in_channels, \n",
    "        num_perceptions=3, \n",
    "        normal_std=0.02, \n",
    "        use_normal_init=True, \n",
    "        zero_bias=True\n",
    "    ):\n",
    "        super(VoxelPerceptionNet, self).__init__()\n",
    "        \n",
    "        self.num_in_channels = num_in_channels\n",
    "        self.normal_std = normal_std\n",
    "        \n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                self.num_in_channels, # incoming channels\n",
    "                self.num_in_channels * num_perceptions, # expand num_in_channels by factor of 3\n",
    "                3, # kernal size of 3, which means neighbour radius of 1\n",
    "                stride=1, # stride of 1 so look at each voxel\n",
    "                padding=1, # make sure to look at edge voxels\n",
    "                groups=self.num_in_channels, # disconnect the perceptions so that multiple percpetions run on each channel; essentially we have num_perceptions convolutional layers side by side\n",
    "                bias=False, # no bias\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # init weights for Conv3d layers\n",
    "                nn.init.normal_(m.weight, std=normal_std)\n",
    "                \n",
    "                # if bias exist, init bias\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    if zero_bias:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                    else:\n",
    "                        nn.init.normal_(m.bias, std=normal_std)\n",
    "\n",
    "        # weight initialisation\n",
    "        if use_normal_init:\n",
    "            with torch.no_grad():\n",
    "                self.apply(init_weights)\n",
    "                \n",
    "    def forward(self, input):\n",
    "        return self.sequence(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = VoxelPerceptionNet(3, normal_std=0.02, num_perceptions=3, use_normal_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence.0.weight tensor([[[[[-2.6198e-03, -1.5750e-02, -3.8632e-02],\n",
      "           [ 3.1331e-02,  1.0204e-02, -1.5932e-02],\n",
      "           [-1.3426e-02,  2.2175e-02,  7.1230e-05]],\n",
      "\n",
      "          [[-2.6037e-02, -1.6551e-02, -2.4201e-02],\n",
      "           [-1.5151e-03, -2.9265e-02,  1.2596e-02],\n",
      "           [ 2.9972e-02,  1.3883e-02, -2.0188e-02]],\n",
      "\n",
      "          [[ 2.2803e-02,  2.0536e-02,  4.6237e-02],\n",
      "           [ 9.9740e-03, -1.2579e-02,  6.9827e-03],\n",
      "           [ 2.3991e-02,  1.4006e-02,  1.1868e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.6638e-02,  1.1156e-02, -1.6559e-02],\n",
      "           [-2.3454e-02,  2.6331e-02,  2.8104e-03],\n",
      "           [ 3.1758e-02,  3.0760e-02,  2.8987e-02]],\n",
      "\n",
      "          [[-2.2075e-02,  2.7824e-02, -7.2198e-04],\n",
      "           [ 1.7351e-02, -1.1078e-02, -1.7559e-02],\n",
      "           [ 9.3460e-03, -2.2787e-02, -2.6558e-03]],\n",
      "\n",
      "          [[ 8.4337e-03, -1.7452e-02,  2.3922e-02],\n",
      "           [ 8.8808e-03, -5.5788e-03, -7.6895e-04],\n",
      "           [ 1.2141e-02, -1.7628e-03, -1.9574e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-8.8659e-03, -2.4839e-02, -2.7438e-02],\n",
      "           [ 3.6888e-03, -3.4741e-02, -8.1345e-03],\n",
      "           [ 2.0924e-02, -4.9798e-03,  3.1842e-02]],\n",
      "\n",
      "          [[ 1.8746e-02, -3.0970e-02,  6.7457e-03],\n",
      "           [ 9.0439e-03, -2.7671e-02,  3.5049e-02],\n",
      "           [ 7.6323e-03,  8.3864e-03,  2.4712e-02]],\n",
      "\n",
      "          [[-2.5131e-03, -4.1002e-03, -1.1431e-02],\n",
      "           [ 3.4135e-02,  7.4111e-03,  9.8763e-03],\n",
      "           [-2.9196e-03,  5.0502e-03,  1.1960e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.1754e-02,  6.2813e-03, -3.5330e-03],\n",
      "           [ 2.3025e-02,  1.9623e-02, -3.6005e-02],\n",
      "           [ 2.5486e-02, -5.2487e-02,  9.5364e-03]],\n",
      "\n",
      "          [[ 9.4257e-03, -2.4135e-02, -1.2226e-02],\n",
      "           [-4.6589e-02, -1.2082e-02,  4.2606e-03],\n",
      "           [-4.5637e-03,  2.3945e-03, -3.9113e-02]],\n",
      "\n",
      "          [[-4.1864e-03, -2.0240e-02,  2.8780e-03],\n",
      "           [-2.6011e-03, -3.5112e-03,  6.8454e-03],\n",
      "           [ 2.4504e-02,  1.9312e-02, -1.2391e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.2575e-02, -6.9492e-03, -2.0562e-02],\n",
      "           [-4.7336e-03, -3.9220e-03, -6.7520e-03],\n",
      "           [-4.9464e-03,  1.3707e-02,  1.7853e-02]],\n",
      "\n",
      "          [[-2.1236e-03,  2.8485e-02, -1.9325e-02],\n",
      "           [ 1.6931e-02,  1.8335e-02, -3.5620e-02],\n",
      "           [-8.8241e-03, -3.7042e-02, -6.2482e-03]],\n",
      "\n",
      "          [[-1.3939e-02, -2.4474e-02, -2.0605e-02],\n",
      "           [ 4.7811e-04, -2.6962e-02,  1.2756e-02],\n",
      "           [-1.4980e-02,  7.5251e-03,  9.2350e-03]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-3.2943e-03, -2.9736e-02, -3.3067e-02],\n",
      "           [-3.6033e-02, -1.0270e-02,  6.9871e-03],\n",
      "           [ 1.3791e-03, -2.5479e-02, -1.6768e-02]],\n",
      "\n",
      "          [[-1.5698e-02, -3.3516e-03,  3.3681e-03],\n",
      "           [ 2.4801e-02, -3.1517e-02, -3.0849e-02],\n",
      "           [-1.4821e-02, -4.2455e-03,  2.4771e-02]],\n",
      "\n",
      "          [[ 9.9878e-04,  1.6461e-02,  1.1963e-02],\n",
      "           [ 3.2967e-02, -3.4139e-04,  4.9532e-03],\n",
      "           [-4.0277e-03,  5.0897e-02, -1.3499e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.5711e-02,  3.3536e-05, -3.0698e-03],\n",
      "           [ 1.4250e-02, -3.2514e-02, -5.1111e-03],\n",
      "           [-2.7949e-03,  1.1528e-02, -1.7774e-02]],\n",
      "\n",
      "          [[-1.7796e-02,  1.1430e-02, -2.6782e-02],\n",
      "           [-1.3212e-02,  1.5104e-02, -7.0322e-03],\n",
      "           [ 1.1825e-02,  1.8461e-02, -1.8336e-02]],\n",
      "\n",
      "          [[-1.4523e-03,  8.4005e-03,  3.1076e-02],\n",
      "           [-4.4984e-03,  2.3764e-02, -8.0750e-03],\n",
      "           [ 3.7380e-03, -1.1550e-02, -1.4052e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 3.8265e-03,  1.2835e-03,  2.2743e-02],\n",
      "           [ 6.0609e-04,  3.6117e-02, -3.4330e-02],\n",
      "           [-2.6292e-03, -8.6775e-03, -1.0074e-02]],\n",
      "\n",
      "          [[ 5.0065e-02,  2.1791e-02, -2.0480e-02],\n",
      "           [-7.8703e-03, -1.0333e-02, -5.1119e-02],\n",
      "           [-3.8113e-02,  2.5501e-03,  5.8929e-03]],\n",
      "\n",
      "          [[ 2.3327e-02, -1.1315e-02, -3.0116e-03],\n",
      "           [ 4.3330e-02, -1.1659e-02, -2.7016e-02],\n",
      "           [ 1.9327e-03, -1.4692e-02,  2.2180e-02]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-5.8391e-03, -1.2991e-02,  1.5306e-02],\n",
      "           [ 2.3604e-02, -7.5315e-03,  1.2556e-02],\n",
      "           [-2.5726e-02,  3.9468e-02,  2.5784e-02]],\n",
      "\n",
      "          [[ 6.9586e-03, -1.1010e-03,  1.8972e-02],\n",
      "           [-1.0662e-02, -2.7318e-02, -1.5173e-02],\n",
      "           [ 1.5927e-02, -3.2519e-02, -1.1846e-02]],\n",
      "\n",
      "          [[ 7.6978e-03, -6.1206e-04,  4.2110e-02],\n",
      "           [-5.7007e-03,  1.1420e-02,  5.3928e-02],\n",
      "           [ 6.0961e-03,  3.0001e-03, -4.6761e-03]]]]])\n"
     ]
    }
   ],
   "source": [
    "# print all params for debug'\n",
    "for name, param in test2.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "                        Kernel Shape      Output Shape  Params  Mult-Adds\n",
      "Layer                                                                    \n",
      "0_sequence.Conv3d_0  [1, 9, 3, 3, 3]  [16, 9, 8, 8, 8]     243     124416\n",
      "-------------------------------------------------------------------------\n",
      "                      Totals\n",
      "Total params             243\n",
      "Trainable params         243\n",
      "Non-trainable params       0\n",
      "Mult-Adds             124416\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = summary(test2, torch.rand(16, 3, 8, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Update Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelUpdateNet(nn.Module):\n",
    "    # This is essentially running dense nets in parallel for each voxel. It takes visual features and predict the update for each feature. In comes the output from a VoxelPerceptionNet, which is of the shape (N, num_in_channels * num_perceptions, channel_dims[0], channel_dims[1], channel_dims[2])\n",
    "    \n",
    "    def __init__(self,\n",
    "        num_channels = 16,\n",
    "        num_perceptions=3,\n",
    "        channel_dims=[64, 64],\n",
    "        normal_std=0.02,\n",
    "        use_normal_init=True,\n",
    "        zero_bias=True,\n",
    "    ):\n",
    "        super(VoxelUpdateNet, self).__init__()\n",
    "        \n",
    "        def make_sequental(num_channels, channel_dims):\n",
    "                \n",
    "            # make first layer. \n",
    "            sequence = [\n",
    "                # visual_feature_channels, x, y, z -> channel_dims[0], x, y, z\n",
    "                nn.Conv3d(num_channels * num_perceptions, channel_dims[0], kernel_size=1), \n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            \n",
    "            # loop through dims[1:] and make Conv3d\n",
    "            for i in range(1, len(channel_dims)):\n",
    "                sequence.extend([\n",
    "                    nn.Conv3d(channel_dims[i - 1], channel_dims[i], kernel_size=1), \n",
    "                    nn.ReLU()\n",
    "                ])\n",
    "                \n",
    "            # make final layer\n",
    "            sequence.extend([\n",
    "                    nn.Conv3d(channel_dims[-1], num_channels, kernel_size=1, bias=False)\n",
    "                ])\n",
    "                        \n",
    "            return nn.Sequential(*sequence)\n",
    "        \n",
    "        self.update_net = make_sequental(num_channels, channel_dims)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                # init weights for Conv3d layers\n",
    "                nn.init.normal_(m.weight, std=normal_std)\n",
    "                \n",
    "                # if bias exist, init bias\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    if zero_bias:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                    else:\n",
    "                        nn.init.normal_(m.bias, std=normal_std)\n",
    "\n",
    "        # weight initialisation\n",
    "        if use_normal_init:\n",
    "            with torch.no_grad():\n",
    "                self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.update_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = VoxelUpdateNet(num_channels = 16, num_perceptions=3, channel_dims=[64, 32], normal_std=0.02, use_normal_init=True, zero_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================\n",
      "                            Kernel Shape      Output Shape  Params  Mult-Adds\n",
      "Layer                                                                        \n",
      "0_update_net.Conv3d_0  [48, 64, 1, 1, 1]  [3, 64, 8, 8, 8]  3.136k  1.572864M\n",
      "1_update_net.ReLU_1                    -  [3, 64, 8, 8, 8]       -          -\n",
      "2_update_net.Conv3d_2  [64, 32, 1, 1, 1]  [3, 32, 8, 8, 8]   2.08k  1.048576M\n",
      "3_update_net.ReLU_3                    -  [3, 32, 8, 8, 8]       -          -\n",
      "4_update_net.Conv3d_4  [32, 16, 1, 1, 1]  [3, 16, 8, 8, 8]   512.0   262.144k\n",
      "-----------------------------------------------------------------------------\n",
      "                         Totals\n",
      "Total params             5.728k\n",
      "Trainable params         5.728k\n",
      "Non-trainable params        0.0\n",
      "Mult-Adds             2.883584M\n",
      "=============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaosarium/opt/anaconda3/lib/python3.9/site-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_sum = df.sum()\n"
     ]
    }
   ],
   "source": [
    "_ = summary(test3, torch.rand(3,48,8,8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VoxelNCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelNCAModel(nn.Module):\n",
    "    def __init__(self,\n",
    "        alpha_living_threshold: float = 0.1, # level below which the cell would be dead\n",
    "        cell_fire_rate: float = 0.5, # how often do cells update\n",
    "        step_size: float = 1.0, # ?\n",
    "        num_perceptions = 3, # num of filters\n",
    "        perception_requires_grad: bool = True, # if perception filters are trainable\n",
    "        num_hidden_channels: int = 127, # num hidden channels\n",
    "        normal_std: float = 0.0002, # for initialisation\n",
    "        use_normal_init: bool = True, # whether to init\n",
    "        zero_bias: bool = True, # whether to init bias as 0s\n",
    "        update_net_channel_dims: List[int] = [32, 32], # channel sizes for hidden layers in VoxelUpdateNet\n",
    "    ):\n",
    "        super(VoxelNCAModel, self).__init__()\n",
    "        self.alpha_living_threshold = alpha_living_threshold\n",
    "        self.cell_fire_rate = cell_fire_rate\n",
    "        self.step_size = step_size\n",
    "        self.num_perceptions = num_perceptions\n",
    "        self.perception_requires_grad = perception_requires_grad\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.normal_std = normal_std\n",
    "        self.use_normal_init = use_normal_init\n",
    "        self.zero_bias = zero_bias\n",
    "        self.update_net_channel_dims = update_net_channel_dims\n",
    "        \n",
    "        # let's have 1 alpha channel + num_hidden_channels * hidden channels\n",
    "        self.alpha_channel_index = 0\n",
    "        self.num_channels = 1 + self.num_hidden_channels\n",
    "        \n",
    "        self.perception_net = VoxelPerceptionNet(\n",
    "            num_in_channels = self.num_channels, \n",
    "            num_perceptions = self.num_perceptions, \n",
    "            normal_std = self.normal_std, \n",
    "            use_normal_init = self.use_normal_init, \n",
    "            zero_bias = self.zero_bias\n",
    "        )\n",
    "        if not self.perception_requires_grad:\n",
    "            for p in self.perception_net.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.update_network = VoxelUpdateNet(\n",
    "            num_channels = self.num_channels,\n",
    "            num_perceptions = self.num_perceptions,\n",
    "            channel_dims = self.update_net_channel_dims,\n",
    "            normal_std = self.normal_std, \n",
    "            use_normal_init = self.use_normal_init, \n",
    "            zero_bias = self.zero_bias\n",
    "        )\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def check_alive(self, state):\n",
    "        # scan the alpha channel and do a max pooling to get the maximum alpha for the cell's neighbourhood\n",
    "        return F.max_pool3d(\n",
    "            # cut out the one-hot block channels from the world (N, channels, x, y, z)\n",
    "            state[:, self.alpha_channel_index : self.alpha_channel_index + 1, :, :, :],\n",
    "            kernel_size = 3,\n",
    "            stride = 1,\n",
    "            padding = 1,\n",
    "        )\n",
    "    \n",
    "    def perceive(self, state):\n",
    "        return self.perception_net(state)\n",
    "    \n",
    "    def update(self, state):\n",
    "        # this is going to result in a boolean tensor indicating cells that are alive\n",
    "        pre_update_mask = self.check_alive(state) > self.alpha_living_threshold\n",
    "        \n",
    "        # extract features using the perception net\n",
    "        perception = self.perceive(state)\n",
    "        # calculate update deltas using the update net\n",
    "        delta = self.update_network(perception)\n",
    "        \n",
    "        # mask out some cells. We take out (N, 1, x, y, z)\n",
    "        rand_mask = torch.rand_like(state[:, 0:1, :, :, :]) < self.cell_fire_rate\n",
    "        # multiply with the delta tensor to mask changes. This will broadcast to all channels\n",
    "        delta = delta * rand_mask.float()\n",
    "        \n",
    "        # now we apply the changes\n",
    "        state = state + delta\n",
    "        \n",
    "        # now get another boolean tensor of cells that are alive after update\n",
    "        post_update_mask = self.check_alive(state) > self.alpha_living_threshold\n",
    "        \n",
    "        # cells are alive if they are alive both before and after update\n",
    "        life_mask = (pre_update_mask & post_update_mask).float()\n",
    "        # make all the dead cells everything zero\n",
    "        state = state * life_mask\n",
    "                \n",
    "        return state, life_mask\n",
    "        \n",
    "    def forward(self, \n",
    "            state, # the world state before update\n",
    "            steps = 1, # how many steps to run the NCA\n",
    "            get_final_mask = False\n",
    "        ):\n",
    "        # in comes a batch of worlds (N, channels, x, y, z)\n",
    "        for step in range(steps):\n",
    "            state, life_mask = self.update(state)\n",
    "        if get_final_mask: return state, life_mask\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_nca_model = VoxelNCAModel(alpha_living_threshold = 0.1,\n",
    "    cell_fire_rate = 0.5,\n",
    "    step_size = 1.0,\n",
    "    num_perceptions = 3,\n",
    "    perception_requires_grad = True,\n",
    "    num_hidden_channels = 127,\n",
    "    normal_std = 0.0002,\n",
    "    use_normal_init = True,\n",
    "    zero_bias = True,\n",
    "    update_net_channel_dims = [32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_state = torch.rand(16, 128, 8, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 8, 8, 8])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state = voxel_nca_model.forward(sample_state, steps = 24)\n",
    "new_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(voxel_nca_model, input_to_model=(sample_state))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a8d7ae5c7f0d87e5344f9e6608bfcb3eb27eaba4a1efb346157a5f4c9c93fcf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
