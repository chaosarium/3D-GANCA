{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6Ma8GkMY4xd"
   },
   "source": [
    "# Training GANCA-3D\n",
    "\n",
    "We run five different experiments:\n",
    "\n",
    "- Use deconvolutional WGAN-GP to as generator. This serves as the baseline\n",
    "- GANCA on MMGAN\n",
    "- GANCA on WGAN\n",
    "- GANCA on WGAN-GP\n",
    "- GANCA on dual-discriminator WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable pytorch tpu spport on colab\n",
    "# !pip --quiet install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "# !pip --quiet install torch==1.9 torchtext==0.10 torchvision==0.10 torchaudio==0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsBYltuhZqzQ",
    "outputId": "ce685181-45ae-41b2-ef4c-6a446818c3ab"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://github.com/chaosarium/3D-GANCA/raw/master/data_helper.py\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/raw/master/utils.py\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/raw/master/models.py\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/raw/master/block_ids_alt.tsv\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/raw/master/output.zip\n",
    "# !unzip -q output.zip\n",
    "# !mkdir dataset\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/blob/master/dataset/filtered_houses_stats.pkl4 -O dataset/filtered_houses_stats.pkl4\n",
    "# !wget -q https://github.com/chaosarium/3D-GANCA/blob/master/dataset/filtered_houses_stats.pkl -O dataset/filtered_houses_stats.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXAbk29ubUCP",
    "outputId": "f295e52e-d03b-4404-fe6a-be7dd410ae0b"
   },
   "outputs": [],
   "source": [
    "# !pip --quiet install torchsummaryX loguru einops pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0AuuMetY4xh",
    "outputId": "7d4500f4-10c3-48bb-931d-6ab0d43dd1d7"
   },
   "outputs": [],
   "source": [
    "# import stuff\n",
    "\n",
    "import data_helper\n",
    "from data_helper import GANCA3DDataModule\n",
    "from models import VoxelPerceptionNet, VoxelUpdateNet, VoxelNCAModel, VoxelDiscriminator, VoxelDeconvGenerator\n",
    "import visualise_helper\n",
    "import utils\n",
    "%load_ext autoreload\n",
    "from tqdm.notebook import tqdm\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "from torchsummaryX import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "\n",
    "from loguru import logger as gurulogger\n",
    "gurulogger.remove()\n",
    "gurulogger.add(sys.stdout, colorize=True, format=\"<blue>{time}</blue> <level>{message}</level>\")\n",
    "gurulogger.level(\"INFO\", color=\"<red><bold>\")\n",
    "\n",
    "import os\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "BLOCK2VEC_OUT_PATH = 'output/block2vec saves/block2vec 64 dim locked air/'\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ulC-44_aY4xk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding, mcid2block, block2embeddingidx, embeddingidx2block, block2mcid = utils.get_embedding_info(BLOCK2VEC_OUT_PATH)\n",
    "\n",
    "converter = utils.DataConverter(embedding, mcid2block, block2embeddingidx, embeddingidx2block, block2mcid)\n",
    "\n",
    "air_embedding = embedding(torch.tensor(converter.block2embeddingidx['minecraft:air']))\n",
    "air_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ps -ef | grep tensorboard | grep -v grep | awk '{print $2}' | xargs kill\n",
    "# !tensorboard --logdir lightning_logs/\n",
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MMGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eELhjPZ1Y4xn"
   },
   "outputs": [],
   "source": [
    "class GANCA_MMGAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr = 2e-4,\n",
    "            beta1 = 0.9,\n",
    "            beta2 = 0.999,\n",
    "            num_embedding_channels = 64,\n",
    "            num_hidden_channels = 63,\n",
    "            update_net_channel_dims = [32, 32],\n",
    "            embedding: torch.nn.Embedding = None,\n",
    "            step_range = [16, 20],\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # call this to save args to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.num_embedding_channels = num_embedding_channels\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.update_net_channel_dims = update_net_channel_dims\n",
    "        # the channels will be like [alpha, embeddings ... , hiddens ...]\n",
    "        self.num_channels = 1 + self.num_embedding_channels + self.num_hidden_channels\n",
    "        self.world_size = (32,32,32)\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad=False # freeze embeddings\n",
    "        self.step_range = step_range\n",
    "        \n",
    "        self.generator = VoxelNCAModel(\n",
    "            alpha_living_threshold = 0.1,\n",
    "            cell_fire_rate = 0.5,\n",
    "            num_perceptions = 3,\n",
    "            perception_requires_grad = True,\n",
    "            num_embedding_channels = self.num_embedding_channels,\n",
    "            num_hidden_channels = self.num_hidden_channels,\n",
    "            normal_std = 0.0002,\n",
    "            use_normal_init = True,\n",
    "            zero_bias = True,\n",
    "            update_net_channel_dims = self.update_net_channel_dims,\n",
    "        )\n",
    "        self.discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = self.num_embedding_channels, \n",
    "            use_sigmoid=True,\n",
    "        )\n",
    "        \n",
    "        # generate some random seeds (N, channels, x, y, z)\n",
    "        self.validation_noise = self.make_seed_states(16)\n",
    "        \n",
    "    def make_seed_states(self, batch_size):\n",
    "        return utils.make_seed_state(\n",
    "            batch_size = batch_size,\n",
    "            num_channels = self.num_channels, \n",
    "            alpha_channel_index = 0,\n",
    "            seed_dim = (4, 4, 4), \n",
    "            world_size = self.world_size,\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "        num_steps = random.randint(*self.step_range)\n",
    "            \n",
    "        real_houses = batch\n",
    "        type_holder = batch[0,0,0,0].to(torch.float) # this is a dummy type for creating labels\n",
    "        size_this_batch = real_houses.shape[0]\n",
    "                \n",
    "        # make noise\n",
    "        \n",
    "        seed_states = self.make_seed_states(size_this_batch).type_as(type_holder) # same batch size as those coming in\n",
    "            \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            \n",
    "            # generate images\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps)\n",
    "                        \n",
    "            # create ground truth result (all fake results) we want D to say the generated ones are real so they are all 1s\n",
    "            real_labels = utils.make_real_labels(size_this_batch).type_as(type_holder)\n",
    "            \n",
    "            # now get the embedding parts out of the fake states\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :]\n",
    "            \n",
    "            # see what discriminator thinks\n",
    "            fake_predictions = self.discriminator.forward(fake_houses)\n",
    "            \n",
    "            # calculate loss\n",
    "            g_loss = F.binary_cross_entropy(fake_predictions, real_labels) # y_hat, y  \n",
    "                                    \n",
    "            self.log(\"g_loss\", g_loss.detach(), prog_bar=True, logger=True)\n",
    "\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # get embeddings for the real house\n",
    "            real_houses = utils.examples2embedding(real_houses, self.embedding)\n",
    "            \n",
    "            # pass in real image to D and try to make it predict all 1s\n",
    "            real_targets = utils.make_real_labels(size_this_batch).type_as(type_holder)\n",
    "            real_predictions = self.discriminator.forward(real_houses)\n",
    "            real_loss = F.binary_cross_entropy(real_predictions, real_targets)\n",
    "            real_acc = torch.sum(real_predictions > 0.5).item()\n",
    "            \n",
    "            # meanwhile, we also want D to be able to tell that outputs from G are all fake\n",
    "            fake_targets = utils.make_fake_labels(size_this_batch).type_as(type_holder)\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps).detach() # detach so that gradients don't pass back into generator\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :] # extract the embedding parts\n",
    "            fake_predictions = self.discriminator.forward(fake_houses)\n",
    "            fake_loss = F.binary_cross_entropy(fake_predictions, fake_targets)\n",
    "            fake_acc = torch.sum(real_predictions < 0.5).item()\n",
    "                        \n",
    "            # discriminator loss is the average of the two losses\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            avg_acc = (real_acc + fake_acc) / 2\n",
    "                    \n",
    "            self.log(\"fake_loss\", fake_loss.detach(), prog_bar=False, logger=True)\n",
    "            self.log(\"real_loss\", real_loss.detach(), prog_bar=False, logger=True)\n",
    "            self.log(\"d_loss\", d_loss.detach(), prog_bar=True, logger=True)\n",
    "            self.log(\"real_acc\", real_acc, prog_bar=False, logger=True)\n",
    "            self.log(\"fake_acc\", fake_acc, prog_bar=False, logger=True)\n",
    "            self.log(\"avg_acc\", avg_acc, prog_bar=True, logger=True)\n",
    "\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self,):\n",
    "        # define pytorch optimizers here. return [list of optimizers], [list of LR schedulers]\n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        return [g_optimizer, d_optimizer], []\n",
    "    \n",
    "    def visualise_gen_process(self, step_size = 4, max_steps = 32):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_steps = math.ceil(max_steps/step_size)\n",
    "\n",
    "            state = self.validation_noise.to(self.device)\n",
    "            snapshots = utils.extract_embedding_channels(state, self.num_embedding_channels)\n",
    "            alpha_snapshots = utils.extract_alpha_channels(state)\n",
    "\n",
    "            for i in tqdm(range(num_steps), desc=\"forward pass for visualisation\", colour='orange', ncols=1000, leave=False):\n",
    "                state = self.generator.forward(state, steps=step_size)\n",
    "                snapshots = torch.cat((snapshots, utils.extract_embedding_channels(state, self.num_embedding_channels)))\n",
    "                alpha_snapshots = torch.cat((alpha_snapshots, utils.extract_alpha_channels(state)))\n",
    "                \n",
    "            self.snapshots_folder_path = self.logger.log_dir + '/gen_snapshots'\n",
    "            try: os.makedirs(self.snapshots_folder_path)\n",
    "            except: pass\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}.npy', snapshots.cpu().numpy())\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}_alpha.npy', alpha_snapshots.cpu().numpy())\n",
    "            \n",
    "    def training_epoch_end(self, training_step_outputs = None):\n",
    "        # make snapshot of progress array\n",
    "        self.visualise_gen_process(step_size=4, max_steps=self.step_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lonFxlJKY4xq"
   },
   "outputs": [],
   "source": [
    "model = GANCA_MMGAN(\n",
    "    lr = 2e-4,\n",
    "    beta1 = 0.9,\n",
    "    beta2 = 0.999,\n",
    "    num_embedding_channels = 64,\n",
    "    num_hidden_channels = 63,\n",
    "    update_net_channel_dims = [32, 32],\n",
    "    embedding = embedding,\n",
    "    step_range = [32, 36],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "2bdc0941b3dc499496679c7183fbff37",
      "5ae9d82c8a094e79920bb1724cfe53e8",
      "96facbd6da40424c93f074bafcb76bbe",
      "9ecea295f69c405c93af3df43b354f75",
      "9a928ccf02ae4c668bfc2be196f31376",
      "42edc229ae79434388cea378717a9aaf",
      "a65f1f8a486f47419cab33d55d86d7f9",
      "67c0303d8a574ebe9e5b35036297b8d5",
      "9060020c253a4429bfadca4b37cb5f2c",
      "e369d4e56d89406f83db778df71ffe05",
      "c6fd5703ce2743948c5decccee2d602b"
     ]
    },
    "id": "9HO0hndNY4xr",
    "outputId": "6324c09c-c795-4879-e4fa-0fa57a25e588"
   },
   "outputs": [],
   "source": [
    "dm = GANCA3DDataModule(batch_size=8, num_workers=NUM_WORKERS, mcid2block = mcid2block, block2embeddingid = block2embeddingidx, debug=False)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CR1dFG7Y4xr",
    "outputId": "8b398d80-b248-4be9-a5ca-3c034df5637b"
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='lightning_logs', name='GANCA_MMGAN', default_hp_metric=False)\n",
    "trainer = Trainer(gpus=1, max_epochs=32, fast_dev_run=False, log_every_n_steps=1, logger=logger, profiler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "ee702aff0c59434fa13204508cb5d22d",
      "c3805b1dda574f99a51de0c5d55b9680",
      "a5a13ea11c644321af52864783858480",
      "d609d2a11ddc48d2b5bfb05303d090d1",
      "726aa9cc89954c079c5f95faffbd4ac9",
      "dbf6830427734779b2205e75544b5d02",
      "d7c23ef46d9e4b8ab6c4086a11d23d1c",
      "c6254117a2a34d21b9880429680304b8",
      "906bb93fea954f17aec435cdbb592bee",
      "9eb0c4b9d0884e53badf26ab9bcab9c0",
      "dba04c61834b49b79f7bfd2b669a99ba"
     ]
    },
    "id": "QdN43RM2Y4xs",
    "outputId": "d8dcb443-6a68-4de7-b08b-018b8f06dd24"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANCA_WGAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr = 2e-4,\n",
    "            n_critic = 5,\n",
    "            # beta1 = 0.9,\n",
    "            # beta2 = 0.999,\n",
    "            weight_clip = 0.01, # glipping gradient in WGAN\n",
    "            num_embedding_channels = 64,\n",
    "            num_hidden_channels = 63,\n",
    "            update_net_channel_dims = [32, 32],\n",
    "            embedding: torch.nn.Embedding = None,\n",
    "            step_range = [16, 20],\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # call this to save args to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.n_critic = n_critic\n",
    "        self.weight_clip = weight_clip\n",
    "        # self.beta1 = beta1\n",
    "        # self.beta2 = beta2\n",
    "        self.num_embedding_channels = num_embedding_channels\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.update_net_channel_dims = update_net_channel_dims\n",
    "        # the channels will be like [alpha, embeddings ... , hiddens ...]\n",
    "        self.num_channels = 1 + self.num_embedding_channels + self.num_hidden_channels\n",
    "        self.world_size = (32,32,32)\n",
    "        self.seed_size = (2,2,2)\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad=False # freeze embeddings\n",
    "        self.step_range = step_range\n",
    "        \n",
    "        self.generator = VoxelNCAModel(\n",
    "            alpha_living_threshold = 0.1,\n",
    "            cell_fire_rate = 0.5,\n",
    "            num_perceptions = 3,\n",
    "            perception_requires_grad = True,\n",
    "            num_embedding_channels = self.num_embedding_channels,\n",
    "            num_hidden_channels = self.num_hidden_channels,\n",
    "            normal_std = 0.0002,\n",
    "            use_normal_init = True,\n",
    "            zero_bias = True,\n",
    "            update_net_channel_dims = self.update_net_channel_dims,\n",
    "        )\n",
    "        self.discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = self.num_embedding_channels, \n",
    "            use_sigmoid=False,\n",
    "        )\n",
    "        \n",
    "        # generate some random seeds (N, channels, x, y, z)\n",
    "        self.validation_noise = self.make_seed_states(4)\n",
    "        \n",
    "    def make_seed_states(self, batch_size):\n",
    "        return utils.make_seed_state(\n",
    "            batch_size = batch_size,\n",
    "            num_channels = self.num_channels, \n",
    "            alpha_channel_index = 0,\n",
    "            seed_dim = self.seed_size, \n",
    "            world_size = self.world_size,\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "                \n",
    "        num_steps = random.randint(*self.step_range)\n",
    "\n",
    "        real_houses_indices = batch\n",
    "        type_holder = batch[0,0,0,0].to(torch.float) # this is a dummy type for creating labels\n",
    "        size_this_batch = real_houses_indices.shape[0]\n",
    "                \n",
    "        # make noise\n",
    "        seed_states = self.make_seed_states(size_this_batch).type_as(type_holder) # same batch size as those coming in\n",
    "            \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "            \n",
    "            # generate fake houses and get the embedding parts out of it\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps)\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :]\n",
    "            \n",
    "            # train gen\n",
    "            g_loss = self.train_generator(fake_houses)              \n",
    "            self.log(\"g_loss\", g_loss.detach(), prog_bar=True, logger=True)\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "                        \n",
    "            # get embeddings for the real house\n",
    "            real_houses = utils.examples2embedding(real_houses_indices, self.embedding)\n",
    "\n",
    "            # generate fake houses\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps).detach() # detach so that gradients don't pass back into generator\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :] # extract the embedding parts\n",
    "            \n",
    "            # train D\n",
    "            d_loss, real_loss, fake_loss = self.train_discriminator(real_houses, fake_houses)\n",
    "\n",
    "            # logging\n",
    "            self.log(\"fake_loss\", fake_loss.detach(), prog_bar=False, logger=True)\n",
    "            self.log(\"real_loss\", real_loss.detach(), prog_bar=False, logger=True)\n",
    "            self.log(\"d_loss\", d_loss.detach(), prog_bar=True, logger=True)\n",
    "\n",
    "            return d_loss\n",
    "\n",
    "    def train_discriminator(self, real_houses, fake_houses):\n",
    "        \n",
    "        # making sure that real and fake have same shape\n",
    "        assert real_houses.shape == fake_houses.shape\n",
    "\n",
    "        # make predictions on real houses and see what D thinks\n",
    "        real_predictions = self.discriminator.forward(real_houses)\n",
    "        # For WGAN, we no longer use binary cross entropy. There is no target here.\n",
    "        real_loss = - torch.mean(real_predictions) # maximising is the same as minimising the negative\n",
    "        \n",
    "        # see what D thinks on fake houses\n",
    "        fake_predictions = self.discriminator.forward(real_houses)\n",
    "        # once again, not BCE here\n",
    "        fake_loss = torch.mean(fake_predictions)\n",
    "        \n",
    "        # make loss the sum\n",
    "        d_loss = real_loss + fake_loss\n",
    "        \n",
    "        # clamp parameters\n",
    "        for param in self.discriminator.parameters():\n",
    "            param.data.clamp_(-self.weight_clip, self.weight_clip)\n",
    "        \n",
    "        return d_loss, real_loss, fake_loss\n",
    "    \n",
    "    def train_generator(self, fake_houses):\n",
    "        \n",
    "        # see what D thinks \n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        \n",
    "        # calc loss. We want to maximise the prediction for this one (only doing so with G's parameters)\n",
    "        g_loss = -torch.mean(fake_predictions)\n",
    "        \n",
    "        return g_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # for WGAN, use RMSprop\n",
    "        g_optimizer = torch.optim.RMSprop(self.generator.parameters(), lr=self.lr)\n",
    "        d_optimizer = torch.optim.RMSprop(self.discriminator.parameters(), lr=self.lr)\n",
    "        \n",
    "        return (\n",
    "            {\"optimizer\": g_optimizer, \"frequency\": 1},\n",
    "            {\"optimizer\": d_optimizer, \"frequency\": self.n_critic}\n",
    "        )\n",
    "        \n",
    "    def visualise_gen_process(self, step_size = 4, max_steps = 32):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_steps = math.ceil(max_steps/step_size)\n",
    "\n",
    "            state = self.validation_noise.to(self.device)\n",
    "            snapshots = utils.extract_embedding_channels(state, self.num_embedding_channels)\n",
    "            alpha_snapshots = utils.extract_alpha_channels(state)\n",
    "\n",
    "            for i in tqdm(range(num_steps), desc=\"forward pass for visualisation\", colour='orange', ncols=1000, leave=False):\n",
    "                state = self.generator.forward(state, steps=step_size)\n",
    "                snapshots = torch.cat((snapshots, utils.extract_embedding_channels(state, self.num_embedding_channels)))\n",
    "                alpha_snapshots = torch.cat((alpha_snapshots, utils.extract_alpha_channels(state)))\n",
    "                \n",
    "            self.snapshots_folder_path = self.logger.log_dir + '/gen_snapshots'\n",
    "            try: os.makedirs(self.snapshots_folder_path)\n",
    "            except: pass\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}.npy', snapshots.cpu().numpy())\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}_alpha.npy', alpha_snapshots.cpu().numpy())\n",
    "            \n",
    "    def training_epoch_end(self, training_step_outputs = None):\n",
    "        # make snapshot of progress array\n",
    "        self.visualise_gen_process(step_size=4, max_steps=self.step_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANCA_WGAN(\n",
    "    lr = 0.00005, # proposed in the WGAN paper\n",
    "    n_critic = 5, # proposed in the WGAN paper\n",
    "    weight_clip = 0.01, # proposed in the WGAN paper\n",
    "    num_embedding_channels = 64,\n",
    "    num_hidden_channels = 63,\n",
    "    update_net_channel_dims = [32, 32],\n",
    "    embedding = embedding,\n",
    "    step_range = [32, 36],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "2bdc0941b3dc499496679c7183fbff37",
      "5ae9d82c8a094e79920bb1724cfe53e8",
      "96facbd6da40424c93f074bafcb76bbe",
      "9ecea295f69c405c93af3df43b354f75",
      "9a928ccf02ae4c668bfc2be196f31376",
      "42edc229ae79434388cea378717a9aaf",
      "a65f1f8a486f47419cab33d55d86d7f9",
      "67c0303d8a574ebe9e5b35036297b8d5",
      "9060020c253a4429bfadca4b37cb5f2c",
      "e369d4e56d89406f83db778df71ffe05",
      "c6fd5703ce2743948c5decccee2d602b"
     ]
    },
    "id": "9HO0hndNY4xr",
    "outputId": "6324c09c-c795-4879-e4fa-0fa57a25e588"
   },
   "outputs": [],
   "source": [
    "dm = GANCA3DDataModule(batch_size=8, num_workers=NUM_WORKERS, mcid2block = mcid2block, block2embeddingid = block2embeddingidx, debug=False)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CR1dFG7Y4xr",
    "outputId": "8b398d80-b248-4be9-a5ca-3c034df5637b"
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='lightning_logs', name='GANCA_WGAN', default_hp_metric=False)\n",
    "trainer = Trainer(gpus=1, max_epochs=32, fast_dev_run=False, log_every_n_steps=1, logger=logger, profiler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "ee702aff0c59434fa13204508cb5d22d",
      "c3805b1dda574f99a51de0c5d55b9680",
      "a5a13ea11c644321af52864783858480",
      "d609d2a11ddc48d2b5bfb05303d090d1",
      "726aa9cc89954c079c5f95faffbd4ac9",
      "dbf6830427734779b2205e75544b5d02",
      "d7c23ef46d9e4b8ab6c4086a11d23d1c",
      "c6254117a2a34d21b9880429680304b8",
      "906bb93fea954f17aec435cdbb592bee",
      "9eb0c4b9d0884e53badf26ab9bcab9c0",
      "dba04c61834b49b79f7bfd2b669a99ba"
     ]
    },
    "id": "QdN43RM2Y4xs",
    "outputId": "d8dcb443-6a68-4de7-b08b-018b8f06dd24"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANCA_WGANGP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr = 0.0001,\n",
    "            alpha_living_threshold = 0.1,\n",
    "            beta1 = 0,\n",
    "            beta2 = 0.9,\n",
    "            n_gen = 1,\n",
    "            n_critic = 5,\n",
    "            lambda_gp = 10,\n",
    "            num_embedding_channels = 64,\n",
    "            num_hidden_channels = 63,\n",
    "            update_net_channel_dims = [32, 32],\n",
    "            embedding: torch.nn.Embedding = None,\n",
    "            converter = None,\n",
    "            step_range = [16, 20],\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # call this to save args to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.alpha_living_threshold = alpha_living_threshold\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.n_gen = n_gen\n",
    "        self.n_critic = n_critic\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.num_embedding_channels = num_embedding_channels\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.update_net_channel_dims = update_net_channel_dims\n",
    "        # the channels will be like [alpha, embeddings ... , hiddens ...]\n",
    "        self.num_channels = 1 + self.num_embedding_channels + self.num_hidden_channels\n",
    "        self.world_size = (32,32,32)\n",
    "        self.seed_size = (2,2,2)\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad=False # freeze embeddings\n",
    "        self.step_range = step_range\n",
    "        self.converter = converter\n",
    "        \n",
    "        self.generator = VoxelNCAModel(\n",
    "            alpha_living_threshold = self.alpha_living_threshold,\n",
    "            cell_fire_rate = 0.5,\n",
    "            num_perceptions = 3,\n",
    "            perception_requires_grad = True,\n",
    "            num_embedding_channels = self.num_embedding_channels,\n",
    "            num_hidden_channels = self.num_hidden_channels,\n",
    "            normal_std = 0.0002,\n",
    "            use_normal_init = True,\n",
    "            zero_bias = True,\n",
    "            update_net_channel_dims = self.update_net_channel_dims,\n",
    "        )\n",
    "        self.discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = self.num_embedding_channels, \n",
    "            use_sigmoid=False,\n",
    "        )\n",
    "        \n",
    "        # generate some random seeds (N, channels, x, y, z)\n",
    "        self.validation_noise = self.make_seed_states(4)\n",
    "        \n",
    "    def make_seed_states(self, batch_size):\n",
    "        return utils.make_seed_state(\n",
    "            batch_size = batch_size,\n",
    "            num_channels = self.num_channels, \n",
    "            alpha_channel_index = 0,\n",
    "            seed_dim = self.seed_size, \n",
    "            world_size = self.world_size,\n",
    "        )\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \n",
    "        # Random weight term for interpolation between real and fake samples. We get a tensor of shape (N, 1, 1, 1, 1)\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1, 1))).to(self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        interpolates = interpolates.to(self.device)\n",
    "        # calc predictions for interpolated samples\n",
    "        interpolates_predictions = self.discriminator.forward(interpolates)\n",
    "        fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).to(self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolates_predictions,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(gradients.size(0), -1).to(self.device)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "        num_steps = random.randint(*self.step_range)\n",
    "\n",
    "        real_houses_indices = batch\n",
    "        type_holder = batch[0,0,0,0].to(torch.float) # this is a dummy type for creating labels\n",
    "        size_this_batch = real_houses_indices.shape[0]\n",
    "                \n",
    "        # make noise\n",
    "        seed_states = self.make_seed_states(size_this_batch).type_as(type_holder) # same batch size as those coming in\n",
    "            \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate fake houses and get the embedding parts out of it\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps)\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :]\n",
    "            \n",
    "            # train gen\n",
    "            g_loss = self.train_generator(fake_houses)  \n",
    "                        \n",
    "            self.log(\"g_loss\", g_loss.detach(), prog_bar=True, logger=True)\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # get embeddings for the real house\n",
    "            real_houses = utils.examples2embedding(real_houses_indices, self.embedding)\n",
    "\n",
    "            # generate fake houses\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps).detach() # detach so that gradients don't pass back into generator\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :] # extract the embedding parts\n",
    "            \n",
    "            # train D\n",
    "            d_loss = self.train_discriminator(real_houses, fake_houses)\n",
    "\n",
    "            # logging\n",
    "            self.log(\"d_loss\", d_loss.detach(), prog_bar=True, logger=True)\n",
    "            return d_loss\n",
    "\n",
    "    def train_discriminator(self, real_houses, fake_houses):\n",
    "        assert real_houses.shape == fake_houses.shape\n",
    "\n",
    "        real_predictions = self.discriminator.forward(real_houses)\n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        self.log(\"real_validity\", torch.mean(real_predictions).detach(), prog_bar=True, logger=True)\n",
    "        self.log(\"fake_validity\", torch.mean(fake_predictions).detach(), prog_bar=True, logger=True)\n",
    "        \n",
    "        gradient_penalty = self.compute_gradient_penalty(real_houses.data, fake_houses.data)\n",
    "                \n",
    "        d_loss = -torch.mean(real_predictions) + torch.mean(fake_predictions) + self.lambda_gp * gradient_penalty\n",
    "                \n",
    "        return d_loss\n",
    "    \n",
    "    def train_generator(self, fake_houses):\n",
    "        # see what D thinks \n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        \n",
    "        # calc loss. We want to maximise the prediction for this one (only doing so with G's parameters)\n",
    "        g_loss = -torch.mean(fake_predictions)\n",
    "        \n",
    "        return g_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        \n",
    "        return [\n",
    "            {\"optimizer\": g_optimizer, \"frequency\": self.n_gen},\n",
    "            {\"optimizer\": d_optimizer, \"frequency\": self.n_critic},\n",
    "        ]\n",
    "        \n",
    "    def visualise_gen_process(self, step_size = 4, max_steps = 32):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_steps = math.ceil(max_steps/step_size)\n",
    "\n",
    "            state = self.validation_noise.to(self.device)\n",
    "            snapshots = utils.extract_embedding_channels(state, self.num_embedding_channels)\n",
    "            alpha_snapshots = utils.extract_alpha_channels(state)\n",
    "\n",
    "            for i in tqdm(range(num_steps), desc=\"forward pass for visualisation\", colour='orange', ncols=1000, leave=False):\n",
    "                state = self.generator.forward(state, steps=step_size)\n",
    "                snapshots = torch.cat((snapshots, utils.extract_embedding_channels(state, self.num_embedding_channels)))\n",
    "                alpha_snapshots = torch.cat((alpha_snapshots, utils.extract_alpha_channels(state)))\n",
    "                \n",
    "            self.snapshots_folder_path = self.logger.log_dir + '/gen_snapshots'\n",
    "            try: os.makedirs(self.snapshots_folder_path)\n",
    "            except: pass\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}.npy', snapshots.cpu().numpy())\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}_alpha.npy', alpha_snapshots.cpu().numpy())\n",
    "            \n",
    "    def training_epoch_end(self, training_step_outputs = None):\n",
    "        # make snapshot of progress array\n",
    "        self.visualise_gen_process(step_size=4, max_steps=self.step_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANCA_WGANGP(\n",
    "    alpha_living_threshold = 0,\n",
    "    lr = 0.0001, # as proposed in WGAN-GP paper\n",
    "    beta1 = 0, # as proposed in WGAN-GP paper\n",
    "    beta2 = 0.9, # as proposed in WGAN-GP paper\n",
    "    n_gen = 2, # huristically 2\n",
    "    n_critic = 3, # huristically 3\n",
    "    lambda_gp = 10,\n",
    "    num_embedding_channels = 64,\n",
    "    num_hidden_channels = 63,\n",
    "    update_net_channel_dims = [32, 32],\n",
    "    embedding = embedding,\n",
    "    converter = converter,\n",
    "    step_range = [32, 36],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "2bdc0941b3dc499496679c7183fbff37",
      "5ae9d82c8a094e79920bb1724cfe53e8",
      "96facbd6da40424c93f074bafcb76bbe",
      "9ecea295f69c405c93af3df43b354f75",
      "9a928ccf02ae4c668bfc2be196f31376",
      "42edc229ae79434388cea378717a9aaf",
      "a65f1f8a486f47419cab33d55d86d7f9",
      "67c0303d8a574ebe9e5b35036297b8d5",
      "9060020c253a4429bfadca4b37cb5f2c",
      "e369d4e56d89406f83db778df71ffe05",
      "c6fd5703ce2743948c5decccee2d602b"
     ]
    },
    "id": "9HO0hndNY4xr",
    "outputId": "6324c09c-c795-4879-e4fa-0fa57a25e588",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = GANCA3DDataModule(batch_size=8, num_workers=NUM_WORKERS, mcid2block = mcid2block, block2embeddingid = block2embeddingidx, debug=False)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CR1dFG7Y4xr",
    "outputId": "8b398d80-b248-4be9-a5ca-3c034df5637b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='lightning_logs', name='GANCA_WGANGP', default_hp_metric=False)\n",
    "trainer = Trainer(gpus=1, max_epochs=32, fast_dev_run=False, log_every_n_steps=1, logger=logger, profiler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "ee702aff0c59434fa13204508cb5d22d",
      "c3805b1dda574f99a51de0c5d55b9680",
      "a5a13ea11c644321af52864783858480",
      "d609d2a11ddc48d2b5bfb05303d090d1",
      "726aa9cc89954c079c5f95faffbd4ac9",
      "dbf6830427734779b2205e75544b5d02",
      "d7c23ef46d9e4b8ab6c4086a11d23d1c",
      "c6254117a2a34d21b9880429680304b8",
      "906bb93fea954f17aec435cdbb592bee",
      "9eb0c4b9d0884e53badf26ab9bcab9c0",
      "dba04c61834b49b79f7bfd2b669a99ba"
     ]
    },
    "id": "QdN43RM2Y4xs",
    "outputId": "d8dcb443-6a68-4de7-b08b-018b8f06dd24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP-Dual-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tt/63g2nkx15fl855gr711x66540000gn/T/ipykernel_93179/4174925212.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGANCA_WGANGP_DUAL_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     def __init__(self,\n\u001b[1;32m      4\u001b[0m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0malpha_living_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class GANCA_WGANGP_DUAL_D(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr = 0.0001,\n",
    "            alpha_living_threshold = 0.1,\n",
    "            beta1 = 0,\n",
    "            beta2 = 0.9,\n",
    "            n_gen = 1,\n",
    "            n_critic = 5,\n",
    "            n_a_critic = 3,\n",
    "            lambda_gp = 10,\n",
    "            num_embedding_channels = 64,\n",
    "            num_hidden_channels = 63,\n",
    "            update_net_channel_dims = [32, 32],\n",
    "            embedding: torch.nn.Embedding = None,\n",
    "            converter = None,\n",
    "            step_range = [16, 20],\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # call this to save args to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.alpha_living_threshold = alpha_living_threshold\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.n_gen = n_gen\n",
    "        self.n_critic = n_critic\n",
    "        self.n_a_critic = n_a_critic\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.num_embedding_channels = num_embedding_channels\n",
    "        self.num_hidden_channels = num_hidden_channels\n",
    "        self.update_net_channel_dims = update_net_channel_dims\n",
    "        # the channels will be like [alpha, embeddings ... , hiddens ...]\n",
    "        self.num_channels = 1 + self.num_embedding_channels + self.num_hidden_channels\n",
    "        self.world_size = (32,32,32)\n",
    "        self.seed_size = (2,2,2)\n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad=False # freeze embeddings\n",
    "        self.step_range = step_range\n",
    "        self.converter = converter\n",
    "        \n",
    "        self.generator = VoxelNCAModel(\n",
    "            alpha_living_threshold = self.alpha_living_threshold,\n",
    "            cell_fire_rate = 0.5,\n",
    "            num_perceptions = 3,\n",
    "            perception_requires_grad = True,\n",
    "            num_embedding_channels = self.num_embedding_channels,\n",
    "            num_hidden_channels = self.num_hidden_channels,\n",
    "            normal_std = 0.0002,\n",
    "            use_normal_init = True,\n",
    "            zero_bias = True,\n",
    "            update_net_channel_dims = self.update_net_channel_dims,\n",
    "            dont_mask_alpha = True,\n",
    "        )\n",
    "        self.discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = self.num_embedding_channels, \n",
    "            use_sigmoid=False,\n",
    "        )\n",
    "        self.aliveness_discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = 1, \n",
    "            use_sigmoid=False,\n",
    "        )\n",
    "        \n",
    "        # generate some random seeds (N, channels, x, y, z)\n",
    "        self.validation_noise = self.make_seed_states(4)\n",
    "        \n",
    "    def make_seed_states(self, batch_size):\n",
    "        return utils.make_seed_state(\n",
    "            batch_size = batch_size,\n",
    "            num_channels = self.num_channels, \n",
    "            alpha_channel_index = 0,\n",
    "            seed_dim = self.seed_size, \n",
    "            world_size = self.world_size,\n",
    "        )\n",
    "    \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \n",
    "        # Random weight term for interpolation between real and fake samples. We get a tensor of shape (N, 1, 1, 1, 1)\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1, 1))).to(self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        interpolates = interpolates.to(self.device)\n",
    "        # calc predictions for interpolated samples\n",
    "        interpolates_predictions = self.discriminator.forward(interpolates)\n",
    "        fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).to(self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolates_predictions,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(gradients.size(0), -1).to(self.device)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def compute_alpha_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \n",
    "        # Random weight term for interpolation between real and fake samples. We get a tensor of shape (N, 1, 1, 1, 1)\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1, 1))).to(self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        interpolates = interpolates.to(self.device)\n",
    "        # calc predictions for interpolated samples\n",
    "        interpolates_predictions = self.aliveness_discriminator.forward(interpolates)\n",
    "        fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).to(self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolates_predictions,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(gradients.size(0), -1).to(self.device)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "        num_steps = random.randint(*self.step_range)\n",
    "\n",
    "        real_houses_indices = batch\n",
    "        type_holder = batch[0,0,0,0].to(torch.float) # this is a dummy type for creating labels\n",
    "        size_this_batch = real_houses_indices.shape[0]\n",
    "                \n",
    "        # make noise\n",
    "        seed_states = self.make_seed_states(size_this_batch).type_as(type_holder) # same batch size as those coming in\n",
    "            \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate fake houses and get the embedding parts out of it\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps)\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :]\n",
    "            fake_alpha = fake_houses_states[:, 0:1, :, :, :]\n",
    "\n",
    "            # train gen\n",
    "            g_loss = self.train_generator(fake_houses, fake_alpha)  \n",
    "                        \n",
    "            self.log(\"g_loss\", g_loss.detach(), prog_bar=True, logger=True)\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # get embeddings for the real house\n",
    "            real_houses = utils.examples2embedding(real_houses_indices, self.embedding)\n",
    "\n",
    "            # generate fake houses\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps).detach() # detach so that gradients don't pass back into generator\n",
    "            fake_houses = fake_houses_states[:, 1:self.num_embedding_channels+1, :, :, :] # extract the embedding parts\n",
    "            \n",
    "            # train D\n",
    "            d_loss = self.train_discriminator(real_houses, fake_houses)\n",
    "\n",
    "            # logging\n",
    "            self.log(\"d_loss\", d_loss.detach(), prog_bar=True, logger=True)\n",
    "            return d_loss\n",
    "        \n",
    "        if optimizer_idx == 2:\n",
    "            # train the aliveness discriminator.\n",
    "            '''\n",
    "            ## Analysis:\n",
    "            \n",
    "            Assigning -1 to all dead cells and 1 to all alive cells in a real example doesn't work because fake alpha is close to 0 by default, so it's very easy for aliveness D\n",
    "            So, what do we do?\n",
    "            We could try to award the generator for generating an alive configuration that look like a real house without punishing it for dead cells\n",
    "            Maybe we can go for a leaky relu!\n",
    "            \n",
    "            Another problem: the empty cells that haven't grown yet are punished for having a 0 alpha while they shouldn't\n",
    "            Proposed solution: mask the real alpha by selecting the grown cells that have a none-zero alpha\n",
    "            \n",
    "            Another problem: there's too much award given to an alpha value close to 1, but all we want is a positive alpha value\n",
    "            Proposed solution: make a concaved down activation so that the difference between 0.1 and 1.0 isn't large\n",
    "            \n",
    "            '''\n",
    "            # generate fake houses\n",
    "            fake_houses_states = self.generator.forward(seed_states, steps=num_steps).detach()\n",
    "            fake_alpha = fake_houses_states[:, 0:1, :, :, :] # extract alpha channel\n",
    "\n",
    "            # get alpha for the real house\n",
    "            real_alpha = torch.zeros(real_houses_indices.shape).type_as(type_holder)\n",
    "            real_alpha = real_alpha + (real_houses_indices == 0) * -1\n",
    "            real_alpha = real_alpha + (real_houses_indices != 0) * 1\n",
    "            real_alpha = real_alpha.reshape(real_alpha.shape[0], 1, real_alpha.shape[1], real_alpha.shape[2], real_alpha.shape[3])\n",
    "            \n",
    "            # train alpha D\n",
    "            d_a_loss = self.train_aliveness_discriminator(real_alpha, fake_alpha)\n",
    "\n",
    "            # logging\n",
    "            self.log(\"d_a_loss\", d_a_loss.detach(), prog_bar=True, logger=True)\n",
    "            return d_a_loss\n",
    "\n",
    "    def train_discriminator(self, real_houses, fake_houses):\n",
    "        assert real_houses.shape == fake_houses.shape\n",
    "\n",
    "        real_predictions = self.discriminator.forward(real_houses)\n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        self.log(\"real_validity\", torch.mean(real_predictions).detach(), prog_bar=True, logger=True)\n",
    "        self.log(\"fake_validity\", torch.mean(fake_predictions).detach(), prog_bar=True, logger=True)\n",
    "        \n",
    "        gradient_penalty = self.compute_gradient_penalty(real_houses.data, fake_houses.data)\n",
    "                \n",
    "        d_loss = -torch.mean(real_predictions) + torch.mean(fake_predictions) + self.lambda_gp * gradient_penalty\n",
    "                \n",
    "        return d_loss\n",
    "    \n",
    "    def train_aliveness_discriminator(self, real_alpha, fake_alpha):\n",
    "        assert real_alpha.shape == fake_alpha.shape\n",
    "\n",
    "        real_predictions = self.aliveness_discriminator.forward(real_alpha)\n",
    "        fake_predictions = self.aliveness_discriminator.forward(fake_alpha)\n",
    "        self.log(\"real_alpha_validity\", torch.mean(real_predictions).detach(), prog_bar=True, logger=True)\n",
    "        self.log(\"fake_alpha_validity\", torch.mean(fake_predictions).detach(), prog_bar=True, logger=True)\n",
    "        \n",
    "        gradient_penalty = self.compute_alpha_gradient_penalty(real_alpha.data, fake_alpha.data)\n",
    "                \n",
    "        d_a_loss = -torch.mean(real_predictions) + torch.mean(fake_predictions) + self.lambda_gp * gradient_penalty\n",
    "                \n",
    "        return d_a_loss\n",
    "    \n",
    "    def train_generator(self, fake_houses, fake_alpha):\n",
    "        # see what D thinks \n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        fake_alpha_predictions = self.aliveness_discriminator.forward(fake_alpha)\n",
    "        \n",
    "        # calc loss. We want to maximise the prediction for this one (only doing so with G's parameters)\n",
    "        # g_loss = - torch.mean(fake_predictions)\n",
    "        g_loss = - (0. * torch.mean(fake_predictions) + 1. * torch.mean(fake_alpha_predictions))\n",
    "        \n",
    "        return g_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_a_optimizer = torch.optim.Adam(self.aliveness_discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        \n",
    "        return [\n",
    "            {\"optimizer\": g_optimizer, \"frequency\": self.n_gen},\n",
    "            {\"optimizer\": d_optimizer, \"frequency\": self.n_critic},\n",
    "            {\"optimizer\": d_a_optimizer, \"frequency\": self.n_a_critic},\n",
    "        ]\n",
    "    \n",
    "    def visualise_gen_process(self, step_size = 4, max_steps = 32):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_steps = math.ceil(max_steps/step_size)\n",
    "\n",
    "            state = self.validation_noise.to(self.device)\n",
    "            snapshots = utils.extract_embedding_channels(state, self.num_embedding_channels)\n",
    "            alpha_snapshots = utils.extract_alpha_channels(state)\n",
    "\n",
    "            for i in tqdm(range(num_steps), desc=\"forward pass for visualisation\", colour='orange', ncols=1000, leave=False):\n",
    "                state = self.generator.forward(state, steps=step_size)\n",
    "                snapshots = torch.cat((snapshots, utils.extract_embedding_channels(state, self.num_embedding_channels)))\n",
    "                alpha_snapshots = torch.cat((alpha_snapshots, utils.extract_alpha_channels(state)))\n",
    "                \n",
    "            self.snapshots_folder_path = self.logger.log_dir + '/gen_snapshots'\n",
    "            try: os.makedirs(self.snapshots_folder_path)\n",
    "            except: pass\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}.npy', snapshots.cpu().numpy())\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}_alpha.npy', alpha_snapshots.cpu().numpy())\n",
    "            \n",
    "    def training_epoch_end(self, training_step_outputs = None):\n",
    "        # make snapshot of progress array\n",
    "        self.visualise_gen_process(step_size=4, max_steps=self.step_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANCA_WGANGP_DUAL_D(\n",
    "    alpha_living_threshold = 0,\n",
    "    lr = 0.0001, # as proposed in WGAN-GP paper\n",
    "    beta1 = 0, # as proposed in WGAN-GP paper\n",
    "    beta2 = 0.9, # as proposed in WGAN-GP paper\n",
    "    n_gen = 2, # huristically 2\n",
    "    n_critic = 3, # huristically 3\n",
    "    n_a_critic = 1, # huristically 1\n",
    "    lambda_gp = 10, # as proposed in WGAN-GP paper\n",
    "    num_embedding_channels = 64,\n",
    "    num_hidden_channels = 63,\n",
    "    update_net_channel_dims = [32, 32],\n",
    "    embedding = embedding,\n",
    "    converter = converter,\n",
    "    step_range = [32, 36],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156,
     "referenced_widgets": [
      "2bdc0941b3dc499496679c7183fbff37",
      "5ae9d82c8a094e79920bb1724cfe53e8",
      "96facbd6da40424c93f074bafcb76bbe",
      "9ecea295f69c405c93af3df43b354f75",
      "9a928ccf02ae4c668bfc2be196f31376",
      "42edc229ae79434388cea378717a9aaf",
      "a65f1f8a486f47419cab33d55d86d7f9",
      "67c0303d8a574ebe9e5b35036297b8d5",
      "9060020c253a4429bfadca4b37cb5f2c",
      "e369d4e56d89406f83db778df71ffe05",
      "c6fd5703ce2743948c5decccee2d602b"
     ]
    },
    "id": "9HO0hndNY4xr",
    "outputId": "6324c09c-c795-4879-e4fa-0fa57a25e588",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dm = GANCA3DDataModule(batch_size=8, num_workers=NUM_WORKERS, mcid2block = mcid2block, block2embeddingid = block2embeddingidx, debug=False)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CR1dFG7Y4xr",
    "outputId": "8b398d80-b248-4be9-a5ca-3c034df5637b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='lightning_logs', name='GANCA_WGANGP_DUAL_D', default_hp_metric=False)\n",
    "trainer = Trainer(gpus=1, max_epochs=32, fast_dev_run=False, log_every_n_steps=1, logger=logger, profiler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "ee702aff0c59434fa13204508cb5d22d",
      "c3805b1dda574f99a51de0c5d55b9680",
      "a5a13ea11c644321af52864783858480",
      "d609d2a11ddc48d2b5bfb05303d090d1",
      "726aa9cc89954c079c5f95faffbd4ac9",
      "dbf6830427734779b2205e75544b5d02",
      "d7c23ef46d9e4b8ab6c4086a11d23d1c",
      "c6254117a2a34d21b9880429680304b8",
      "906bb93fea954f17aec435cdbb592bee",
      "9eb0c4b9d0884e53badf26ab9bcab9c0",
      "dba04c61834b49b79f7bfd2b669a99ba"
     ]
    },
    "id": "QdN43RM2Y4xs",
    "outputId": "d8dcb443-6a68-4de7-b08b-018b8f06dd24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP Deconv Generator Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = VoxelDeconvGenerator(\n",
    "    latent_dim = 512,\n",
    "    world_shape = (32,32,32),\n",
    "    num_embedding_channels = 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 32, 32, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = generator(torch.rand(4, 512, 1, 1, 1)).detach()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deconv_WGANGP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "            lr = 0.0001,\n",
    "            beta1 = 0,\n",
    "            beta2 = 0.9,\n",
    "            n_gen = 1,\n",
    "            n_critic = 5,\n",
    "            lambda_gp = 10,\n",
    "            latent_dim = 512,\n",
    "            embedding: torch.nn.Embedding = None,\n",
    "            num_embedding_channels = 64,\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        # call this to save args to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.n_gen = n_gen\n",
    "        self.n_critic = n_critic\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_embedding_channels = num_embedding_channels\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.embedding.weight.requires_grad=False # freeze embeddings\n",
    "\n",
    "        self.world_size = (32,32,32)\n",
    "        \n",
    "        generator = VoxelDeconvGenerator(\n",
    "            latent_dim = self.latent_dim,\n",
    "            world_shape = self.world_size,\n",
    "            num_embedding_channels = self.num_embedding_channels,\n",
    "        )\n",
    "\n",
    "        self.discriminator = VoxelDiscriminator(\n",
    "            num_in_channels = self.num_embedding_channels, \n",
    "            use_sigmoid=False,\n",
    "        )\n",
    "        \n",
    "        # generate some random noise (N, self.latent_dim, 1, 1, 1)\n",
    "        self.validation_noise = torch.rand(4, self.latent_dim, 1, 1, 1)\n",
    "            \n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \n",
    "        # Random weight term for interpolation between real and fake samples. We get a tensor of shape (N, 1, 1, 1, 1)\n",
    "        alpha = torch.Tensor(np.random.random((real_samples.size(0), 1, 1, 1, 1))).to(self.device)\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        interpolates = interpolates.to(self.device)\n",
    "        # calc predictions for interpolated samples\n",
    "        interpolates_predictions = self.discriminator.forward(interpolates)\n",
    "        fake = torch.Tensor(real_samples.shape[0], 1).fill_(1.0).to(self.device)\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=interpolates_predictions,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.reshape(gradients.size(0), -1).to(self.device)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "        real_houses_indices = batch\n",
    "        type_holder = batch[0,0,0,0].to(torch.float) # this is a dummy type for creating labels\n",
    "        size_this_batch = real_houses_indices.shape[0]\n",
    "                \n",
    "        # make noise\n",
    "        noise = torch.rand(size_this_batch, self.latent_dim, 1, 1, 1).type_as(type_holder)\n",
    "            \n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate fake houses and get the embedding parts out of it\n",
    "            fake_houses = self.generator.forward(noise)\n",
    "            \n",
    "            # train gen\n",
    "            g_loss = self.train_generator(fake_houses)  \n",
    "                        \n",
    "            self.log(\"g_loss\", g_loss.detach(), prog_bar=True, logger=True)\n",
    "            return g_loss\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # get embeddings for the real house\n",
    "            real_houses = utils.examples2embedding(real_houses_indices, self.embedding)\n",
    "\n",
    "            # generate fake houses\n",
    "            fake_houses = self.generator.forward(noise).detach()\n",
    "            \n",
    "            # train D\n",
    "            d_loss = self.train_discriminator(real_houses, fake_houses)\n",
    "\n",
    "            # logging\n",
    "            self.log(\"d_loss\", d_loss.detach(), prog_bar=True, logger=True)\n",
    "            return d_loss\n",
    "\n",
    "    def train_discriminator(self, real_houses, fake_houses):\n",
    "        assert real_houses.shape == fake_houses.shape\n",
    "\n",
    "        real_predictions = self.discriminator.forward(real_houses)\n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        self.log(\"real_validity\", torch.mean(real_predictions).detach(), prog_bar=True, logger=True)\n",
    "        self.log(\"fake_validity\", torch.mean(fake_predictions).detach(), prog_bar=True, logger=True)\n",
    "        \n",
    "        gradient_penalty = self.compute_gradient_penalty(real_houses.data, fake_houses.data)\n",
    "                \n",
    "        d_loss = -torch.mean(real_predictions) + torch.mean(fake_predictions) + self.lambda_gp * gradient_penalty\n",
    "                \n",
    "        return d_loss\n",
    "    \n",
    "    def train_generator(self, fake_houses):\n",
    "        # see what D thinks \n",
    "        fake_predictions = self.discriminator.forward(fake_houses)\n",
    "        \n",
    "        # calc loss. We want to maximise the prediction for this one (only doing so with G's parameters)\n",
    "        g_loss = -torch.mean(fake_predictions)\n",
    "        \n",
    "        return g_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        g_optimizer = torch.optim.Adam(self.generator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        d_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n",
    "        \n",
    "        return [\n",
    "            {\"optimizer\": g_optimizer, \"frequency\": self.n_gen},\n",
    "            {\"optimizer\": d_optimizer, \"frequency\": self.n_critic},\n",
    "        ]\n",
    "        \n",
    "    def visualise_gen_process(self, step_size = 4, max_steps = 32):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            num_steps = math.ceil(max_steps/step_size)\n",
    "\n",
    "            state = self.validation_noise.to(self.device)\n",
    "            snapshots = utils.extract_embedding_channels(state, self.num_embedding_channels)\n",
    "            alpha_snapshots = utils.extract_alpha_channels(state)\n",
    "\n",
    "            for i in tqdm(range(num_steps), desc=\"forward pass for visualisation\", colour='orange', ncols=1000, leave=False):\n",
    "                state = self.generator.forward(state, steps=step_size)\n",
    "                snapshots = torch.cat((snapshots, utils.extract_embedding_channels(state, self.num_embedding_channels)))\n",
    "                alpha_snapshots = torch.cat((alpha_snapshots, utils.extract_alpha_channels(state)))\n",
    "                \n",
    "            self.snapshots_folder_path = self.logger.log_dir + '/gen_snapshots'\n",
    "            try: os.makedirs(self.snapshots_folder_path)\n",
    "            except: pass\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}.npy', snapshots.cpu().numpy())\n",
    "            np.save(self.snapshots_folder_path + f'/epoch_{self.current_epoch}_alpha.npy', alpha_snapshots.cpu().numpy())\n",
    "            \n",
    "    def training_epoch_end(self, training_step_outputs = None):\n",
    "        # make snapshot of progress array\n",
    "        self.visualise_gen_process(step_size=4, max_steps=self.step_range[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deconv_WGANGP(\n",
    "    lr = 0.0001, # as proposed in WGAN-GP paper\n",
    "    beta1 = 0, # as proposed in WGAN-GP paper\n",
    "    beta2 = 0.9, # as proposed in WGAN-GP paper\n",
    "    n_gen = 2, # huristically 2\n",
    "    n_critic = 3, # huristically 3\n",
    "    lambda_gp = 10,\n",
    "    num_embedding_channels = 64,\n",
    "    embedding = embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038115dbc1e84d3d80290952bee4279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1977 houses\n",
      "Turning MC id into embedding idx. This could take up to a minute.\n",
      "Done with that.\n"
     ]
    }
   ],
   "source": [
    "dm = GANCA3DDataModule(batch_size=8, num_workers=NUM_WORKERS, mcid2block = mcid2block, block2embeddingid = block2embeddingidx, debug=False)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='lightning_logs', name='Deconv_WGANGP', default_hp_metric=False)\n",
    "trainer = Trainer(gpus=1, max_epochs=32, fast_dev_run=False, log_every_n_steps=1, logger=logger, profiler=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of GANCA train.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "0228fee4d25496bb422a7e0c363e636da60e8258e4b70dda8036e2defd0af2d7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bdc0941b3dc499496679c7183fbff37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_96facbd6da40424c93f074bafcb76bbe",
       "IPY_MODEL_9ecea295f69c405c93af3df43b354f75",
       "IPY_MODEL_9a928ccf02ae4c668bfc2be196f31376"
      ],
      "layout": "IPY_MODEL_5ae9d82c8a094e79920bb1724cfe53e8"
     }
    },
    "42edc229ae79434388cea378717a9aaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ae9d82c8a094e79920bb1724cfe53e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67c0303d8a574ebe9e5b35036297b8d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "726aa9cc89954c079c5f95faffbd4ac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dba04c61834b49b79f7bfd2b669a99ba",
      "placeholder": "",
      "style": "IPY_MODEL_9eb0c4b9d0884e53badf26ab9bcab9c0",
      "value": " 100/200 [03:06&lt;03:06,  1.86s/it, loss=5.49, v_num=0, g_loss=10.90, d_loss=3.16e-5, avg_acc=1.000]"
     }
    },
    "9060020c253a4429bfadca4b37cb5f2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "906bb93fea954f17aec435cdbb592bee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96facbd6da40424c93f074bafcb76bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a65f1f8a486f47419cab33d55d86d7f9",
      "placeholder": "",
      "style": "IPY_MODEL_42edc229ae79434388cea378717a9aaf",
      "value": "100%"
     }
    },
    "9a928ccf02ae4c668bfc2be196f31376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6fd5703ce2743948c5decccee2d602b",
      "placeholder": "",
      "style": "IPY_MODEL_e369d4e56d89406f83db778df71ffe05",
      "value": " 1977/1977 [00:01&lt;00:00, 1319.86it/s]"
     }
    },
    "9eb0c4b9d0884e53badf26ab9bcab9c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ecea295f69c405c93af3df43b354f75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9060020c253a4429bfadca4b37cb5f2c",
      "max": 1977,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_67c0303d8a574ebe9e5b35036297b8d5",
      "value": 1977
     }
    },
    "a5a13ea11c644321af52864783858480": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7c23ef46d9e4b8ab6c4086a11d23d1c",
      "placeholder": "",
      "style": "IPY_MODEL_dbf6830427734779b2205e75544b5d02",
      "value": "Epoch 0:  50%"
     }
    },
    "a65f1f8a486f47419cab33d55d86d7f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3805b1dda574f99a51de0c5d55b9680": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "c6254117a2a34d21b9880429680304b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6fd5703ce2743948c5decccee2d602b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d609d2a11ddc48d2b5bfb05303d090d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_906bb93fea954f17aec435cdbb592bee",
      "max": 200,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6254117a2a34d21b9880429680304b8",
      "value": 100
     }
    },
    "d7c23ef46d9e4b8ab6c4086a11d23d1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dba04c61834b49b79f7bfd2b669a99ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbf6830427734779b2205e75544b5d02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e369d4e56d89406f83db778df71ffe05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee702aff0c59434fa13204508cb5d22d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5a13ea11c644321af52864783858480",
       "IPY_MODEL_d609d2a11ddc48d2b5bfb05303d090d1",
       "IPY_MODEL_726aa9cc89954c079c5f95faffbd4ac9"
      ],
      "layout": "IPY_MODEL_c3805b1dda574f99a51de0c5d55b9680"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
