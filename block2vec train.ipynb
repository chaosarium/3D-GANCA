{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed:\n",
    "\n",
    "- data_helper.py\n",
    "- dataset/filtered_houses_stats.pkl\n",
    "- block_ids_alt.tsv\n",
    "\n",
    "Install on Colab:\n",
    "\n",
    "- einops\n",
    "- umap-learn\n",
    "- fuzzywuzzy\n",
    "- loguru\n",
    "- pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwf_nq9bKJOU"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/chaosarium/3D-GANCA/master/data_helper.py\n",
    "# !wget https://raw.githubusercontent.com/chaosarium/3D-GANCA/master/block_ids_alt.tsv\n",
    "# !wget https://github.com/chaosarium/3D-GANCA/raw/master/dataset/filtered_houses_stats.pkl4 -P dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWX5CvljTVcI"
   },
   "outputs": [],
   "source": [
    "# enable pytorch tpu spport\n",
    "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "# !pip install torch==1.9 torchtext==0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP3p4eP0Mnyq"
   },
   "outputs": [],
   "source": [
    "# !mkdir output\n",
    "# !mkdir output/block2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woRPzjy8Ev8O"
   },
   "outputs": [],
   "source": [
    "# !pip install loguru fuzzywuzzy umap-learn==0.5.1 einops pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp \n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "def download():\n",
    "    data_dir = 'dataset'\n",
    "    url = \"https://craftassist.s3-us-west-2.amazonaws.com/pubr/house_data.tar.gz\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    tar_path = osp.join(data_dir, \"houses.tar.gz\")\n",
    "    if not osp.isfile(tar_path):\n",
    "        print(f\"Downloading dataset from {url}\")\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to retrieve image from url: {url}. \"\n",
    "                f\"Status: {response.status_code}\"\n",
    "            )\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    extracted_dir = osp.join(data_dir, \"house_data\")\n",
    "    if not osp.isdir(extracted_dir):\n",
    "        print(f\"Extracting dataset to {extracted_dir}\")\n",
    "        tar = tarfile.open(tar_path, \"r\")\n",
    "        tar.extractall(data_dir)\n",
    "\n",
    "# download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "import importlib\n",
    "import data_helper\n",
    "# importlib.reload(data_helper)\n",
    "from einops import rearrange\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "import umap.umap_ as umap\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from mpl_toolkits.mplot3d import proj3d, Axes3D\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, colorize=True, format=\"<blue>{time}</blue> <level>{message}</level>\")\n",
    "logger.level(\"INFO\", color=\"<red><bold>\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnnotations3D:\n",
    "    def __init__(self, xyz, imgs: List[np.ndarray], labels: List[str], ax3d: Axes3D, figure: Figure):\n",
    "        self.xyz = xyz\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.ax3d = ax3d\n",
    "        self.figure = figure\n",
    "        self.annot = []\n",
    "        for xyz, im, label in zip(self.xyz, self.imgs, self.labels):\n",
    "            x, y = self.proj(xyz)\n",
    "            self.annot.append(self.image(im, [x, y]))\n",
    "            self.annot.append(self.label(label, [x, y]))\n",
    "        self.lim = self.ax3d.get_w_lims()\n",
    "        self.rot = self.ax3d.get_proj()\n",
    "        self.cid = self.ax3d.figure.canvas.mpl_connect(\n",
    "            \"draw_event\", self.update)\n",
    "\n",
    "        self.funcmap = {\"button_press_event\": self.ax3d._button_press,\n",
    "                        \"motion_notify_event\": self.ax3d._on_move,\n",
    "                        \"button_release_event\": self.ax3d._button_release}\n",
    "\n",
    "        self.cfs = [self.ax3d.figure.canvas.mpl_connect(kind, self.cb)\n",
    "                    for kind in self.funcmap.keys()]\n",
    "\n",
    "    def cb(self, event):\n",
    "        event.inaxes = self.ax3d\n",
    "        self.funcmap[event.name](event)\n",
    "\n",
    "    def proj(self, X):\n",
    "        \"\"\" From a 3D point in axes ax1, \n",
    "            calculate position in 2D in ax2 \"\"\"\n",
    "        x, y, z = X\n",
    "        x2, y2, _ = proj3d.proj_transform(x, y, z, self.ax3d.get_proj())\n",
    "        return x2, y2\n",
    "\n",
    "    def image(self, arr, xy):\n",
    "        \"\"\" Place an image (arr) as annotation at position xy \"\"\"\n",
    "        im = offsetbox.OffsetImage(arr)\n",
    "        ab = offsetbox.AnnotationBbox(im, xy, pad=0)\n",
    "        self.ax3d.add_artist(ab)\n",
    "        return ab\n",
    "\n",
    "    def label(self, label, xy):\n",
    "        text = offsetbox.TextArea(label) # minimumdescent=False\n",
    "        ab = offsetbox.AnnotationBbox(text, xy,\n",
    "                                      xybox=(0, 16),\n",
    "                                      xycoords='data',\n",
    "                                      boxcoords=\"offset points\")\n",
    "        self.ax3d.add_artist(ab)\n",
    "        return ab\n",
    "\n",
    "    def update(self, event):\n",
    "        if np.any(self.ax3d.get_w_lims() != self.lim) or \\\n",
    "                np.any(self.ax3d.get_proj() != self.rot):\n",
    "            self.lim = self.ax3d.get_w_lims()\n",
    "            self.rot = self.ax3d.get_proj()\n",
    "            for s, ab in zip(self.xyz, self.annot):\n",
    "                ab.xy = self.proj(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.target_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "        initrange = 1.0 / self.embedding_dim\n",
    "        init.uniform_(self.target_embeddings.weight.data, -\n",
    "                      initrange, initrange)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        emb_target = self.target_embeddings(target)\n",
    "        # print('current embedding: ', testskipgrammodel.target_embeddings.weight)\n",
    "\n",
    "        score = self.output(emb_target)\n",
    "        # print('score: ', score)\n",
    "        score = F.log_softmax(score, dim=-1)\n",
    "        # print('softmax score: ', score)\n",
    "\n",
    "        losses = torch.stack([F.nll_loss(score, context_word)\n",
    "                              for context_word in context.transpose(0, 1)])\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2VecDataset(Dataset):\n",
    "    def __init__(self, neighbor_radius = 1, block_ids_table_path='block_ids_alt.tsv'):\n",
    "        super().__init__()\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.block_ids_table_path = block_ids_table_path\n",
    "        \n",
    "        self._gen_block_id_lookup_dict()\n",
    "        self._read_blocks()\n",
    "        \n",
    "        padding = 2 * self.neighbor_radius  # one token on each side\n",
    "        self.x_dim = self.x_lims[1] - self.x_lims[0] + 1 - padding\n",
    "        self.y_dim = self.y_lims[1] - self.y_lims[0] + 1 - padding\n",
    "        self.z_dim = self.z_lims[1] - self.z_lims[0] + 1 - padding\n",
    "\n",
    "        # assuming 32*32*32\n",
    "        # print(self._idx_to_coords(0)) # lower bound\n",
    "        # print(self._idx_to_coords(1799)) # should be 2, 30, 30\n",
    "        # print(self._idx_to_coords(26999)) # upper bound\n",
    "        \n",
    "        # print(self._get_neighbors(23,5,5, neighbor_radius=self.neighbor_radius))\n",
    "        \n",
    "        # print(self._getitem(3))\n",
    "        # print(self._getitem(35))\n",
    "        # print(self._getitem(355))\n",
    "    \n",
    "    def _read_size(self, neighbor_radius=1):\n",
    "        return [neighbor_radius, self.world.shape[0] - neighbor_radius - 1], [neighbor_radius, self.world.shape[1] - neighbor_radius - 1], [neighbor_radius, self.world.shape[2] - neighbor_radius - 1]\n",
    "    \n",
    "    def _gen_block_id_lookup_dict(self):\n",
    "        mc_block_database = pd.read_csv('block_ids_alt.tsv', sep='\\t')\n",
    "        mc_block_database = mc_block_database.filter(items=['numerical id', 'item id'])\n",
    "        mc_block_database = mc_block_database.dropna(subset=[\"numerical id\"])\n",
    "        mc_block_database\n",
    "        self.block_id_lookup_dict = mc_block_database.set_index('numerical id').to_dict()['item id']\n",
    "    \n",
    "    def _read_blocks(self):\n",
    "        self.world = data_helper.all_trainx_as_df()[:10] # only take first 100 for now\n",
    "        self.world = self.world['world']\n",
    "        self.world = np.stack(self.world, axis=0)\n",
    "        self.world = rearrange(self.world, 'n x y z b -> (n x) y z b')\n",
    "        self.world = self.world[:,:,:,0]\n",
    "        logger.info(f\"Loaded in world with shape: {self.world.shape}\")\n",
    "        \n",
    "        self.x_lims, self.y_lims, self.z_lims = self._read_size(neighbor_radius=0)\n",
    "        \n",
    "        self.block_frequency = defaultdict(int)\n",
    "        coordinates_to_track = self._gen_coords(*(self._read_size(neighbor_radius=0)))\n",
    "        logger.info(\"Collecting {} blocks for frequency calculation\", len(coordinates_to_track))\n",
    "        for coord in tqdm(coordinates_to_track):\n",
    "            numerical_id = self._get_block(coord[0], coord[1], coord[2])\n",
    "            # treating all meta of same id the same for simplicity\n",
    "                \n",
    "            item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                \n",
    "            self.block_frequency[item_id] += 1\n",
    "\n",
    "        logger.info(f\"Found {len(self.block_frequency)} unique blocks\")\n",
    "        self.block2idx = dict()\n",
    "        self.idx2block = dict()\n",
    "        for name, count in self.block_frequency.items():\n",
    "            block_idx = len(self.block2idx)\n",
    "            self.block2idx[name] = block_idx\n",
    "            self.idx2block[block_idx] = name\n",
    "        logger.info(\"idx2block and block2idx dictionaries generated\")\n",
    "        # print(self.block2idx)\n",
    "        # print(self.idx2block)\n",
    "        \n",
    "    def _get_block(self, x, y, z):\n",
    "        # returns the id for the block\n",
    "        return self.world[x][y][z]\n",
    "\n",
    "    def _get_neighbors(self, x, y, z, neighbor_radius=1):\n",
    "        neighbor_coords = [(x + x_diff, y + y_diff, z + z_diff) for x_diff, y_diff, z_diff in product(list(range(-neighbor_radius, neighbor_radius + 1)), repeat=3) if x_diff != 0 or y_diff != 0 or z_diff != 0]\n",
    "        return [self._get_block(*coord) for coord in neighbor_coords]\n",
    "\n",
    "    def _gen_coords(self, x_lims, y_lims, z_lims):\n",
    "        return [(x, y, z) for x, y, z in product(range(x_lims[0], x_lims[1] + 1), range(y_lims[0], y_lims[1] + 1), range(z_lims[0], z_lims[1] + 1))]\n",
    "    \n",
    "    def _idx_to_coords(self, index):\n",
    "        z = index % (self.z_dim)\n",
    "        y = int(((index - z) / (self.z_dim)) % (self.y_dim))\n",
    "        x = int(((index - z) / (self.z_dim) - y) / (self.y_dim))\n",
    "        x += self.x_lims[0] + self.neighbor_radius\n",
    "        y += self.y_lims[0] + self.neighbor_radius\n",
    "        z += self.z_lims[0] + self.neighbor_radius\n",
    "        return x, y, z\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_dim * self.y_dim * self.z_dim\n",
    "    \n",
    "    def _getitem(self, index):\n",
    "        coords = self._idx_to_coords(index)\n",
    "        numerical_id = self._get_block(*coords)\n",
    "        item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "        target = self.block2idx[item_id]\n",
    "        target = torch.tensor(int(target))\n",
    "        \n",
    "        neighbors = self._get_neighbors(*coords)\n",
    "        item_ids = [self.block_id_lookup_dict[str(numerical_id)] for numerical_id in neighbors]\n",
    "        context = [self.block2idx[item_id] for item_id in item_ids]\n",
    "        \n",
    "        context = torch.tensor(context)\n",
    "        \n",
    "        # discard samples that are all air\n",
    "        if item_id == 'minecraft:air' and item_ids == ['minecraft:air']*len(item_ids):\n",
    "            return self._getitem(np.random.randint(self.__len__()))\n",
    "\n",
    "        return target, context\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._getitem(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2VecDatasetV2(Dataset):\n",
    "    # updated dataset with preprocessing to get rid of unwanted air\n",
    "    def __init__(self, neighbor_radius = 1, block_ids_table_path='block_ids_alt.tsv', preprocess = True, save_preprocessed = True, load_preprocessed = False):\n",
    "        super().__init__()\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.block_ids_table_path = block_ids_table_path\n",
    "        \n",
    "        self._gen_block_id_lookup_dict()\n",
    "        \n",
    "        if preprocess == load_preprocessed == False:\n",
    "            print('hey plz either preprocess or load preprocessed')\n",
    "            raise\n",
    "        \n",
    "        if load_preprocessed:\n",
    "            logger.info(f\"Loading preprocessed block2vec dataset from file\")\n",
    "            try:\n",
    "                self.preprocessed_dataset = np.load('dataset/block2vec_preprocessed.npy', allow_pickle=True)\n",
    "            except:\n",
    "                print('no preprocessed file to load')\n",
    "                raise\n",
    "            \n",
    "        if preprocess:\n",
    "            logger.info(f\"Preprocessing block2vec dataset\")\n",
    "            self._read_blocks()\n",
    "            self._preprpcess()\n",
    "        \n",
    "        if save_preprocessed:\n",
    "            logger.info(f\"Saving preprocessed block2vec dataset to file\")\n",
    "            np.save('dataset/block2vec_preprocessed.npy', self.preprocessed_dataset)\n",
    "        \n",
    "        padding = 2 * self.neighbor_radius  # one token on each side\n",
    "        self.x_dim = self.x_lims[1] - self.x_lims[0] + 1 - padding\n",
    "        self.y_dim = self.y_lims[1] - self.y_lims[0] + 1 - padding\n",
    "        self.z_dim = self.z_lims[1] - self.z_lims[0] + 1 - padding\n",
    "    \n",
    "    # generate size limits for reading self.world with given neighbor radius\n",
    "    def _read_size(self, neighbor_radius=1):\n",
    "        return [neighbor_radius, self.world.shape[0] - neighbor_radius - 1], [neighbor_radius, self.world.shape[1] - neighbor_radius - 1], [neighbor_radius, self.world.shape[2] - neighbor_radius - 1]\n",
    "    \n",
    "    # generate dictionary for converting block id to names\n",
    "    def _gen_block_id_lookup_dict(self):\n",
    "        mc_block_database = pd.read_csv('block_ids_alt.tsv', sep='\\t')\n",
    "        mc_block_database = mc_block_database.filter(items=['numerical id', 'item id'])\n",
    "        mc_block_database = mc_block_database.dropna(subset=[\"numerical id\"])\n",
    "        mc_block_database\n",
    "        self.block_id_lookup_dict = mc_block_database.set_index('numerical id').to_dict()['item id']\n",
    "    \n",
    "    # loop through and register all blocks to get block2idx and idx2block i.e. create vocabulary\n",
    "    def _read_blocks(self):\n",
    "        self.world = data_helper.all_trainx_as_df() # [:10] # only take first 100 for now\n",
    "        self.world = self.world['world']\n",
    "        self.world = np.stack(self.world, axis=0)\n",
    "        self.world = rearrange(self.world, 'n x y z b -> (n x) y z b')\n",
    "        self.world = self.world[:,:,:,0]\n",
    "        logger.info(f\"Loaded in world with shape: {self.world.shape}\")\n",
    "        \n",
    "        self.x_lims, self.y_lims, self.z_lims = self._read_size(neighbor_radius=0)\n",
    "        \n",
    "        self.block_frequency = defaultdict(int)\n",
    "        coordinates_to_track = self._gen_coords(*(self._read_size(neighbor_radius=0)))\n",
    "        logger.info(\"Collecting {} blocks for frequency calculation\", len(coordinates_to_track))\n",
    "        for coords in tqdm(coordinates_to_track):\n",
    "            numerical_id = self._get_block(coords[0], coords[1], coords[2])\n",
    "            # treating all meta of same id the same for simplicity\n",
    "                \n",
    "            item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                \n",
    "            self.block_frequency[item_id] += 1\n",
    "\n",
    "        logger.info(f\"Found {len(self.block_frequency)} unique blocks\")\n",
    "        self.block2idx = dict()\n",
    "        self.idx2block = dict()\n",
    "        for name, count in self.block_frequency.items():\n",
    "            block_idx = len(self.block2idx)\n",
    "            self.block2idx[name] = block_idx\n",
    "            self.idx2block[block_idx] = name\n",
    "        logger.info(\"idx2block and block2idx dictionaries generated\")\n",
    "        \n",
    "    # return the block id at x y z coords\n",
    "    def _get_block(self, x, y, z):\n",
    "        # returns the id for the block\n",
    "        return self.world[x][y][z]\n",
    "\n",
    "    # return a list of block ids of neighboring blocks\n",
    "    def _get_neighbors(self, x, y, z, neighbor_radius=1):\n",
    "        neighbor_coords = [(x + x_diff, y + y_diff, z + z_diff) for x_diff, y_diff, z_diff in product(list(range(-neighbor_radius, neighbor_radius + 1)), repeat=3) if x_diff != 0 or y_diff != 0 or z_diff != 0]\n",
    "        return [self._get_block(*coord) for coord in neighbor_coords]\n",
    "\n",
    "    # generate the full list of coords within limits\n",
    "    def _gen_coords(self, x_lims, y_lims, z_lims):\n",
    "        return [(x, y, z) for x, y, z in product(range(x_lims[0], x_lims[1] + 1), range(y_lims[0], y_lims[1] + 1), range(z_lims[0], z_lims[1] + 1))]\n",
    "    \n",
    "    # [legacy] return x y z for index in readable places\n",
    "    def _idx_to_coords(self, index):\n",
    "        z = index % (self.z_dim)\n",
    "        y = int(((index - z) / (self.z_dim)) % (self.y_dim))\n",
    "        x = int(((index - z) / (self.z_dim) - y) / (self.y_dim))\n",
    "        x += self.x_lims[0] + self.neighbor_radius\n",
    "        y += self.y_lims[0] + self.neighbor_radius\n",
    "        z += self.z_lims[0] + self.neighbor_radius\n",
    "        return x, y, z\n",
    "    \n",
    "    # make target context pairs while ignoring all air places\n",
    "    def _preprpcess(self):\n",
    "        air_removed = 0\n",
    "        preprocessed_dataset = [] # list of [target, contex]\n",
    "        \n",
    "        x_lims, y_lims, z_lims = self._read_size(neighbor_radius=self.neighbor_radius)\n",
    "        coordinates_to_preprocess = self._gen_coords(*(self._read_size(neighbor_radius=self.neighbor_radius)))\n",
    "        logger.info(\"Preprocessing {} blocks for target context pairs creation\", len(coordinates_to_preprocess))\n",
    "        \n",
    "        for coords in tqdm(coordinates_to_preprocess):\n",
    "            numerical_id = self._get_block(coords[0], coords[1], coords[2])\n",
    "            neighbors_numerical_ids = self._get_neighbors(*coords)\n",
    "            \n",
    "            if numerical_id == 0 and neighbors_numerical_ids == [0]*len(neighbors_numerical_ids):\n",
    "                # remove and log if too much air\n",
    "                air_removed += 1\n",
    "            else:\n",
    "                # if not airy add to dataset\n",
    "                item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                target = self.block2idx[item_id]\n",
    "                target = torch.tensor(int(target))\n",
    "                \n",
    "                neighbors_item_ids = [self.block_id_lookup_dict[str(numerical_id)] for numerical_id in neighbors_numerical_ids]\n",
    "                context = [self.block2idx[item_id] for item_id in neighbors_item_ids]\n",
    "                context = torch.tensor(context)\n",
    "                \n",
    "                preprocessed_dataset.append((target, context))\n",
    "        \n",
    "        # turn into np array\n",
    "        self.preprocessed_dataset = np.array(preprocessed_dataset)\n",
    "                \n",
    "        logger.info(f\"Removed {air_removed} examples that are too airy.\")\n",
    "        logger.info(f\"Generated {self.__len__()} example data preprocessed dataset.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.preprocessed_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target, context = self.preprocessed_dataset[index]\n",
    "        return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2Vec(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim = 32, initial_lr = 1e-3, neighbor_radius = 1, batch_size = 256, num_epochs = 30, preprocess = True, save_preprocessed = True, load_preprocessed = False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # making lightning save params under self.hparams\n",
    "        \n",
    "        self.dataset = Block2VecDatasetV2(neighbor_radius=neighbor_radius, preprocess=preprocess, save_preprocessed=save_preprocessed, load_preprocessed=load_preprocessed)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = initial_lr # initial learning rate\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.output_path = \"output/block2vec\"\n",
    "\n",
    "        self.num_embeddings = len(self.dataset.block2idx)\n",
    "        self.model = SkipGramModel(self.num_embeddings, self.embedding_dim)\n",
    "        self.textures = dict()\n",
    "        \n",
    "        # print(self.model)\n",
    "        \n",
    "    def forward(self, target, context) -> torch.Tensor:\n",
    "        return self.model(target, context)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.forward(*batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            math.ceil(len(self.dataset) / self.batch_size) *\n",
    "            self.num_epochs,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        embedding_dict = self.save_embedding(self.dataset.idx2block, self.output_path)\n",
    "        embedding_dict = self.save_embedding(\n",
    "            self.dataset.idx2block, self.output_path\n",
    "        )\n",
    "        self.create_confusion_matrix(\n",
    "            self.dataset.idx2block, self.output_path)\n",
    "        # self.plot_embeddings(embedding_dict, self.output_path)\n",
    "\n",
    "    def read_texture(self, block: str):\n",
    "        if block not in self.textures and block != \"air\":\n",
    "            texture_candidates = Path(\n",
    "                \"/home/schubert/projects/TOAD-GAN/minecraft/block2vec/textures\"\n",
    "            ).glob(\"*.png\")\n",
    "            # use of absolute path is intentional\n",
    "            match = process.extractOne(block, texture_candidates)\n",
    "            if match is not None:\n",
    "                logger.info(\"Matches {} with {} texture file\", block, match[0])\n",
    "                self.textures[block] = plt.imread(match[0])\n",
    "        if block not in self.textures:\n",
    "            self.textures[block] = np.ones(shape=[16, 16, 3])\n",
    "        return self.textures[block]\n",
    "\n",
    "    def save_embedding(self, id2block: Dict[int, str], output_path: str):\n",
    "        embeddings = self.model.target_embeddings.weight\n",
    "        # embeddings = embeddings / torch.norm(embeddings, p=2, dim=-1, keepdim=True)\n",
    "        embeddings = embeddings.cpu().data.numpy()\n",
    "        embedding_dict = {}\n",
    "        with open(os.path.join(output_path, \"embeddings.txt\"), \"w\") as f:\n",
    "            f.write(\"%d %d\\n\" % (len(id2block), self.embedding_dim))\n",
    "            for wid, w in id2block.items():\n",
    "                e = \" \".join(map(lambda x: str(x), embeddings[wid]))\n",
    "                embedding_dict[w] = torch.from_numpy(embeddings[wid])\n",
    "                f.write(\"%s %s\\n\" % (w, e))\n",
    "        np.save(os.path.join(output_path, \"embeddings.npy\"), embeddings)\n",
    "        with open(os.path.join(output_path, f\"representations.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(embedding_dict, f)\n",
    "        return embedding_dict\n",
    "    \n",
    "    def plot_embeddings(self, embedding_dict: Dict[str, np.ndarray], output_path: str):\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        legend = [label.replace(\"minecraft:\", \"\")\n",
    "                  for label in embedding_dict.keys()]\n",
    "        texture_imgs = [self.read_texture(block) for block in legend]\n",
    "        embeddings = torch.stack(list(embedding_dict.values())).numpy()\n",
    "        if embeddings.shape[-1] != 3:\n",
    "            embeddings_3d = umap.UMAP(\n",
    "                n_neighbors=5, min_dist=0.3, n_components=3\n",
    "            ).fit_transform(embeddings)\n",
    "        else:\n",
    "            embeddings_3d = embeddings\n",
    "        for embedding in embeddings_3d:\n",
    "            ax.scatter(*embedding, alpha=0)\n",
    "        ia = ImageAnnotations3D(embeddings_3d, texture_imgs, legend, ax, fig)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, \"scatter_3d.png\"), dpi=300)\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    def create_confusion_matrix(self, id2block: Dict[int, str], output_path: str):\n",
    "        rcParams.update({\"font.size\": 6})\n",
    "        names = []\n",
    "        dists = np.zeros((len(id2block), len(id2block)))\n",
    "        for i, b1 in id2block.items():\n",
    "            names.append(b1.split(\":\")[1])\n",
    "            for j, b2 in id2block.items():\n",
    "                dists[i, j] = F.mse_loss(\n",
    "                    self.model.target_embeddings.weight.data[i],\n",
    "                    self.model.target_embeddings.weight.data[j],\n",
    "                )\n",
    "        confusion_display = ConfusionMatrixDisplay(dists, display_labels=names)\n",
    "        confusion_display.plot(include_values=False,\n",
    "                               xticks_rotation=\"vertical\")\n",
    "        confusion_display.ax_.set_xlabel(\"\")\n",
    "        confusion_display.ax_.set_ylabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, \"dist_matrix.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 32\n",
    "def main():\n",
    "    block2vec = Block2Vec(embedding_dim = 64, initial_lr = 2e-3, neighbor_radius = 1, batch_size = 256, num_epochs = NUM_EPOCHS, preprocess = False, save_preprocessed = False, load_preprocessed = True)\n",
    "    trainer = pl.Trainer(gpus=0, max_epochs=NUM_EPOCHS, fast_dev_run=False)\n",
    "    trainer.fit(block2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-02-13T18:04:23.944514+0800\u001b[0m \u001b[31m\u001b[1mLoading preprocessed block2vec dataset from file\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb97c89c7d33bf00e68473e7beaa192cc5ff3ebaf4fd59b3c63e5a73e8b6a1d4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('world-gan-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
