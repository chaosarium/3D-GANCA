{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "import data_helper\n",
    "import importlib\n",
    "from einops import rearrange\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, colorize=True, format=\"<blue>{time}</blue> <level>{message}</level>\")\n",
    "logger.level(\"INFO\", color=\"<red><bold>\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torchviz import make_dot\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.target_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "        initrange = 1.0 / self.embedding_dim\n",
    "        init.uniform_(self.target_embeddings.weight.data, -\n",
    "                      initrange, initrange)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        emb_target = self.target_embeddings(target)\n",
    "        # print('current embedding: ', testskipgrammodel.target_embeddings.weight)\n",
    "\n",
    "        score = self.output(emb_target)\n",
    "        # print('score: ', score)\n",
    "        score = F.log_softmax(score, dim=-1)\n",
    "        # print('softmax score: ', score)\n",
    "\n",
    "        losses = torch.stack([F.nll_loss(score, context_word)\n",
    "                              for context_word in context.transpose(0, 1)])\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2VecDataset(Dataset):\n",
    "    def __init__(self, neighbor_radius = 1, block_ids_table_path='block_ids_alt.tsv'):\n",
    "        super().__init__()\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.block_ids_table_path = block_ids_table_path\n",
    "        \n",
    "        self._gen_block_id_lookup_dict()\n",
    "        self._read_blocks()\n",
    "        \n",
    "        padding = 2 * self.neighbor_radius  # one token on each side\n",
    "        self.x_dim = self.x_lims[1] - self.x_lims[0] + 1 - padding\n",
    "        self.y_dim = self.y_lims[1] - self.y_lims[0] + 1 - padding\n",
    "        self.z_dim = self.z_lims[1] - self.z_lims[0] + 1 - padding\n",
    "\n",
    "        # assuming 32*32*32\n",
    "        # print(self._idx_to_coords(0)) # lower bound\n",
    "        # print(self._idx_to_coords(1799)) # should be 2, 30, 30\n",
    "        # print(self._idx_to_coords(26999)) # upper bound\n",
    "        \n",
    "        # print(self._get_neighbors(23,5,5, neighbor_radius=self.neighbor_radius))\n",
    "        \n",
    "        # print(self._getitem(3))\n",
    "        # print(self._getitem(35))\n",
    "        # print(self._getitem(355))\n",
    "    \n",
    "    def _read_size(self, neighbor_radius=1):\n",
    "        return [neighbor_radius, self.world.shape[0] - neighbor_radius - 1], [neighbor_radius, self.world.shape[1] - neighbor_radius - 1], [neighbor_radius, self.world.shape[2] - neighbor_radius - 1]\n",
    "    \n",
    "    def _gen_block_id_lookup_dict(self):\n",
    "        mc_block_database = pd.read_csv('block_ids_alt.tsv', sep='\\t')\n",
    "        mc_block_database = mc_block_database.filter(items=['numerical id', 'item id'])\n",
    "        mc_block_database = mc_block_database.dropna(subset=[\"numerical id\"])\n",
    "        mc_block_database\n",
    "        self.block_id_lookup_dict = mc_block_database.set_index('numerical id').to_dict()['item id']\n",
    "    \n",
    "    def _read_blocks(self):\n",
    "        self.world = data_helper.all_trainx_as_df()[:1] # only take first 100 for now\n",
    "        self.world = self.world['world']\n",
    "        self.world = np.stack(self.world, axis=0)\n",
    "        self.world = rearrange(self.world, 'n x y z b -> (n x) y z b')\n",
    "        self.world = self.world[:,:,:,0]\n",
    "        logger.info(f\"Loaded in world with shape: {self.world.shape}\")\n",
    "        \n",
    "        self.x_lims, self.y_lims, self.z_lims = self._read_size(neighbor_radius=0)\n",
    "        \n",
    "        self.block_frequency = defaultdict(int)\n",
    "        coordinates_to_track = self._gen_coords(*(self._read_size(neighbor_radius=0)))\n",
    "        logger.info(\"Collecting {} blocks for frequency calculation\", len(coordinates_to_track))\n",
    "        for coord in tqdm(coordinates_to_track):\n",
    "            numerical_id = self._get_block(coord[0], coord[1], coord[2])\n",
    "            # treating all meta of same id the same for simplicity\n",
    "                \n",
    "            item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                \n",
    "            self.block_frequency[item_id] += 1\n",
    "\n",
    "        logger.info(\"Found {len(self.block_frequency)} unique blocks\")\n",
    "        self.block2idx = dict()\n",
    "        self.idx2block = dict()\n",
    "        for name, count in self.block_frequency.items():\n",
    "            block_idx = len(self.block2idx)\n",
    "            self.block2idx[name] = block_idx\n",
    "            self.idx2block[block_idx] = name\n",
    "        logger.info(\"idx2block and block2idx dictionaries generated\")\n",
    "        # print(self.block2idx)\n",
    "        # print(self.idx2block)\n",
    "        \n",
    "    def _get_block(self, x, y, z):\n",
    "        # returns the id for the block\n",
    "        return self.world[x][y][z]\n",
    "\n",
    "    def _get_neighbors(self, x, y, z, neighbor_radius=1):\n",
    "        neighbor_coords = [(x + x_diff, y + y_diff, z + z_diff) for x_diff, y_diff, z_diff in product(list(range(-neighbor_radius, neighbor_radius + 1)), repeat=3) if x_diff != 0 or y_diff != 0 or z_diff != 0]\n",
    "        return [self._get_block(*coord) for coord in neighbor_coords]\n",
    "\n",
    "    def _gen_coords(self, x_lims, y_lims, z_lims):\n",
    "        return [(x, y, z) for x, y, z in product(range(x_lims[0], x_lims[1] + 1), range(y_lims[0], y_lims[1] + 1), range(z_lims[0], z_lims[1] + 1))]\n",
    "    \n",
    "    def _idx_to_coords(self, index):\n",
    "        z = index % (self.z_dim)\n",
    "        y = int(((index - z) / (self.z_dim)) % (self.y_dim))\n",
    "        x = int(((index - z) / (self.z_dim) - y) / (self.y_dim))\n",
    "        x += self.x_lims[0] + self.neighbor_radius\n",
    "        y += self.y_lims[0] + self.neighbor_radius\n",
    "        z += self.z_lims[0] + self.neighbor_radius\n",
    "        return x, y, z\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x_dim * self.y_dim * self.z_dim\n",
    "    \n",
    "    def _getitem(self, index):\n",
    "        coords = self._idx_to_coords(index)\n",
    "        numerical_id = self._get_block(*coords)\n",
    "        item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "        target = self.block2idx[item_id]\n",
    "        target = torch.tensor(int(target))\n",
    "        \n",
    "        neighbors = self._get_neighbors(*coords)\n",
    "        item_ids = [self.block_id_lookup_dict[str(numerical_id)] for numerical_id in neighbors]\n",
    "        context = [self.block2idx[item_id] for item_id in item_ids]\n",
    "        \n",
    "        context = torch.tensor(context)\n",
    "        return target, context\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._getitem(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2Vec(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim = 32, initial_lr = 1e-3, neighbor_radius = 1, batch_size = 256, num_epochs = 30):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # making lightning save params under self.hparams\n",
    "        \n",
    "        self.dataset = Block2VecDataset(neighbor_radius)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = initial_lr # initial learning rate\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        self.num_embeddings = len(self.dataset.block2idx)\n",
    "        self.model = SkipGramModel(self.num_embeddings, self.embedding_dim)\n",
    "        self.textures = dict()\n",
    "        \n",
    "        print(self.model)\n",
    "        \n",
    "    def forward(self, target, context) -> torch.Tensor:\n",
    "        return self.model(target, context)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.forward(*batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            math.ceil(len(self.dataset) / self.batch_size) *\n",
    "            self.num_epochs,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        print(\"yay!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    block2vec = Block2Vec()\n",
    "    trainer = pl.Trainer(gpus=0, max_epochs=10, fast_dev_run=False)\n",
    "    trainer.fit(block2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc5242ce669441fb8298d814c13aefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1977 houses\n",
      "\u001b[34m2022-02-13T00:39:16.526929+0800\u001b[0m \u001b[31m\u001b[1mLoaded in world with shape: (32, 32, 32)\u001b[0m\n",
      "\u001b[34m2022-02-13T00:39:16.530448+0800\u001b[0m \u001b[31m\u001b[1mCollecting 32768 blocks for frequency calculation\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899da6880e4f44f2a22783052df30e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32768 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-02-13T00:39:16.590568+0800\u001b[0m \u001b[31m\u001b[1mFound {len(self.block_frequency)} unique blocks\u001b[0m\n",
      "\u001b[34m2022-02-13T00:39:16.591022+0800\u001b[0m \u001b[31m\u001b[1midx2block and block2idx dictionaries generated\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | SkipGramModel | 260   \n",
      "----------------------------------------\n",
      "260       Trainable params\n",
      "0         Non-trainable params\n",
      "260       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "/Users/leonlu-m1/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGramModel(\n",
      "  (target_embeddings): Embedding(4, 32)\n",
      "  (output): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n",
      "Epoch 0: 100%|██████████| 106/106 [00:02<00:00, 42.82it/s, loss=0.735, v_num=12]yay!\n",
      "Epoch 1: 100%|██████████| 106/106 [00:01<00:00, 61.80it/s, loss=0.161, v_num=12]yay!\n",
      "Epoch 2: 100%|██████████| 106/106 [00:02<00:00, 48.93it/s, loss=0.107, v_num=12]yay!\n",
      "Epoch 3: 100%|██████████| 106/106 [00:02<00:00, 48.95it/s, loss=0.104, v_num=12]yay!\n",
      "Epoch 4: 100%|██████████| 106/106 [00:02<00:00, 49.62it/s, loss=0.0956, v_num=12]yay!\n",
      "Epoch 5: 100%|██████████| 106/106 [00:02<00:00, 42.19it/s, loss=0.0938, v_num=12]yay!\n",
      "Epoch 6: 100%|██████████| 106/106 [00:02<00:00, 43.55it/s, loss=0.0901, v_num=12]yay!\n",
      "Epoch 7: 100%|██████████| 106/106 [00:02<00:00, 42.64it/s, loss=0.0888, v_num=12]yay!\n",
      "Epoch 8:   1%|          | 1/106 [00:00<00:03, 28.13it/s, loss=0.0891, v_num=12]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonlu-m1/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb97c89c7d33bf00e68473e7beaa192cc5ff3ebaf4fd59b3c63e5a73e8b6a1d4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('world-gan-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
