{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed:\n",
    "\n",
    "- data_helper.py\n",
    "- dataset/filtered_houses_stats.pkl\n",
    "- block_ids_alt.tsv\n",
    "\n",
    "Install on Colab:\n",
    "\n",
    "- einops\n",
    "- umap-learn\n",
    "- fuzzywuzzy\n",
    "- loguru\n",
    "- pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fwf_nq9bKJOU"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/chaosarium/3D-GANCA/master/data_helper.py\n",
    "# !wget https://raw.githubusercontent.com/chaosarium/3D-GANCA/master/block_ids_alt.tsv\n",
    "# !wget https://github.com/chaosarium/3D-GANCA/raw/master/dataset/filtered_houses_stats.pkl4 -P dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xWX5CvljTVcI"
   },
   "outputs": [],
   "source": [
    "# enable pytorch tpu spport\n",
    "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "# !pip install torch==1.9 torchtext==0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP3p4eP0Mnyq"
   },
   "outputs": [],
   "source": [
    "# !mkdir output\n",
    "# !mkdir output/block2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woRPzjy8Ev8O"
   },
   "outputs": [],
   "source": [
    "# !pip install loguru fuzzywuzzy umap-learn==0.5.1 einops pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp \n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "def download():\n",
    "    data_dir = 'dataset'\n",
    "    url = \"https://craftassist.s3-us-west-2.amazonaws.com/pubr/house_data.tar.gz\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    tar_path = osp.join(data_dir, \"houses.tar.gz\")\n",
    "    if not osp.isfile(tar_path):\n",
    "        print(f\"Downloading dataset from {url}\")\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to retrieve image from url: {url}. \"\n",
    "                f\"Status: {response.status_code}\"\n",
    "            )\n",
    "        with open(tar_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    extracted_dir = osp.join(data_dir, \"house_data\")\n",
    "    if not osp.isdir(extracted_dir):\n",
    "        print(f\"Extracting dataset to {extracted_dir}\")\n",
    "        tar = tarfile.open(tar_path, \"r\")\n",
    "        tar.extractall(data_dir)\n",
    "\n",
    "# download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "import importlib\n",
    "import data_helper\n",
    "# importlib.reload(data_helper)\n",
    "from einops import rearrange\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from tqdm.notebook import tqdm\n",
    "import umap.umap_ as umap\n",
    "from pathlib import Path\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from mpl_toolkits.mplot3d import proj3d, Axes3D\n",
    "from matplotlib import offsetbox\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from loguru import logger\n",
    "logger.remove()\n",
    "logger.add(sys.stdout, colorize=True, format=\"<blue>{time}</blue> <level>{message}</level>\")\n",
    "logger.level(\"INFO\", color=\"<red><bold>\")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "# from torchviz import make_dot\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAnnotations3D:\n",
    "    def __init__(self, xyz, imgs: List[np.ndarray], labels: List[str], ax3d: Axes3D, figure: Figure):\n",
    "        self.xyz = xyz\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.ax3d = ax3d\n",
    "        self.figure = figure\n",
    "        self.annot = []\n",
    "        for xyz, im, label in zip(self.xyz, self.imgs, self.labels):\n",
    "            x, y = self.proj(xyz)\n",
    "            self.annot.append(self.image(im, [x, y]))\n",
    "            self.annot.append(self.label(label, [x, y]))\n",
    "        self.lim = self.ax3d.get_w_lims()\n",
    "        self.rot = self.ax3d.get_proj()\n",
    "        self.cid = self.ax3d.figure.canvas.mpl_connect(\n",
    "            \"draw_event\", self.update)\n",
    "\n",
    "        self.funcmap = {\"button_press_event\": self.ax3d._button_press,\n",
    "                        \"motion_notify_event\": self.ax3d._on_move,\n",
    "                        \"button_release_event\": self.ax3d._button_release}\n",
    "\n",
    "        self.cfs = [self.ax3d.figure.canvas.mpl_connect(kind, self.cb)\n",
    "                    for kind in self.funcmap.keys()]\n",
    "\n",
    "    def cb(self, event):\n",
    "        event.inaxes = self.ax3d\n",
    "        self.funcmap[event.name](event)\n",
    "\n",
    "    def proj(self, X):\n",
    "        \"\"\" From a 3D point in axes ax1, \n",
    "            calculate position in 2D in ax2 \"\"\"\n",
    "        x, y, z = X\n",
    "        x2, y2, _ = proj3d.proj_transform(x, y, z, self.ax3d.get_proj())\n",
    "        return x2, y2\n",
    "\n",
    "    def image(self, arr, xy):\n",
    "        \"\"\" Place an image (arr) as annotation at position xy \"\"\"\n",
    "        im = offsetbox.OffsetImage(arr)\n",
    "        ab = offsetbox.AnnotationBbox(im, xy, pad=0)\n",
    "        self.ax3d.add_artist(ab)\n",
    "        return ab\n",
    "\n",
    "    def label(self, label, xy):\n",
    "        text = offsetbox.TextArea(label) # minimumdescent=False\n",
    "        ab = offsetbox.AnnotationBbox(text, xy,\n",
    "                                      xybox=(0, 16),\n",
    "                                      xycoords='data',\n",
    "                                      boxcoords=\"offset points\")\n",
    "        self.ax3d.add_artist(ab)\n",
    "        return ab\n",
    "\n",
    "    def update(self, event):\n",
    "        if np.any(self.ax3d.get_w_lims() != self.lim) or \\\n",
    "                np.any(self.ax3d.get_proj() != self.rot):\n",
    "            self.lim = self.ax3d.get_w_lims()\n",
    "            self.rot = self.ax3d.get_proj()\n",
    "            for s, ab in zip(self.xyz, self.annot):\n",
    "                ab.xy = self.proj(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.target_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, num_embeddings)\n",
    "\n",
    "        initrange = 1.0 / self.embedding_dim\n",
    "        init.uniform_(self.target_embeddings.weight.data, -\n",
    "                      initrange, initrange)\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        emb_target = self.target_embeddings(target)\n",
    "        # print('current embedding: ', testskipgrammodel.target_embeddings.weight)\n",
    "\n",
    "        score = self.output(emb_target)\n",
    "        # print('score: ', score)\n",
    "        score = F.log_softmax(score, dim=-1)\n",
    "        # print('softmax score: ', score)\n",
    "\n",
    "        losses = torch.stack([F.nll_loss(score, context_word)\n",
    "                              for context_word in context.transpose(0, 1)])\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2VecDatasetV2(Dataset):\n",
    "    # updated dataset with preprocessing to get rid of unwanted air\n",
    "    def __init__(self, neighbor_radius = 1, block_ids_table_path='block_ids_alt.tsv', preprocess = True, save_preprocessed = True, load_preprocessed = False):\n",
    "        super().__init__()\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.block_ids_table_path = block_ids_table_path\n",
    "        \n",
    "        self._gen_block_id_lookup_dict()\n",
    "        \n",
    "        if preprocess == load_preprocessed == False:\n",
    "            print('hey plz either preprocess or load preprocessed')\n",
    "            raise\n",
    "        \n",
    "        if load_preprocessed:\n",
    "            logger.info(f\"Loading preprocessed block2vec dataset from file\")\n",
    "            try:\n",
    "                self.preprocessed_dataset = np.load('dataset/block2vec_preprocessed.npy', allow_pickle=True)\n",
    "                with open(\"output/block2vec/block2idx.pkl\", 'rb') as f:\n",
    "                    self.block2idx = pickle.load(f)\n",
    "                with open(\"output/block2vec/idx2block.pkl\", 'rb') as f:\n",
    "                    self.idx2block = pickle.load(f)\n",
    "                with open(\"output/block2vec/block_frequency.pkl\", 'rb') as f:\n",
    "                    self.block_frequency = pickle.load(f)\n",
    "                logger.info(f\"Data loaded\")\n",
    "            except:\n",
    "                print(\"fail to load. files don't exist?\")\n",
    "                raise\n",
    "            \n",
    "        if preprocess:\n",
    "            logger.info(f\"Preprocessing block2vec dataset\")\n",
    "            self._read_blocks()\n",
    "            self._preprpcess()\n",
    "        \n",
    "        if save_preprocessed:\n",
    "            logger.info(f\"Saving preprocessed block2vec dataset to file\")\n",
    "            np.save('dataset/block2vec_preprocessed.npy', self.preprocessed_dataset)\n",
    "            logger.info(\"Saving idx2block and block2idx\")\n",
    "            with open(os.path.join(\"output/block2vec/block2idx.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(self.block2idx, f)\n",
    "            with open(os.path.join(\"output/block2vec/idx2block.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(self.idx2block, f)\n",
    "            with open(os.path.join(\"output/block2vec/block_frequency.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(self.block_frequency, f)\n",
    "\n",
    "        padding = 2 * self.neighbor_radius  # one token on each side\n",
    "\n",
    "    # generate size limits for reading self.world with given neighbor radius\n",
    "    def _read_size(self, neighbor_radius=1):\n",
    "        return [neighbor_radius, self.world.shape[0] - neighbor_radius - 1], [neighbor_radius, self.world.shape[1] - neighbor_radius - 1], [neighbor_radius, self.world.shape[2] - neighbor_radius - 1]\n",
    "    \n",
    "    # generate dictionary for converting block id to names\n",
    "    def _gen_block_id_lookup_dict(self):\n",
    "        mc_block_database = pd.read_csv('block_ids_alt.tsv', sep='\\t')\n",
    "        mc_block_database = mc_block_database.filter(items=['numerical id', 'item id'])\n",
    "        mc_block_database = mc_block_database.dropna(subset=[\"numerical id\"])\n",
    "        mc_block_database\n",
    "        self.block_id_lookup_dict = mc_block_database.set_index('numerical id').to_dict()['item id']\n",
    "    \n",
    "    # loop through and register all blocks to get block2idx and idx2block i.e. create vocabulary\n",
    "    def _read_blocks(self):\n",
    "        self.world = data_helper.all_trainx_as_df()# [:3] # only take first 100 for now\n",
    "        self.world = self.world['world']\n",
    "        self.world = np.stack(self.world, axis=0)\n",
    "        self.world = rearrange(self.world, 'n x y z b -> (n x) y z b')\n",
    "        self.world = self.world[:,:,:,0]\n",
    "        logger.info(f\"Loaded in world with shape: {self.world.shape}\")\n",
    "        \n",
    "        self.x_lims, self.y_lims, self.z_lims = self._read_size(neighbor_radius=0)\n",
    "        \n",
    "        self.block_frequency = defaultdict(int)\n",
    "        coordinates_to_track = self._gen_coords(*(self._read_size(neighbor_radius=0)))\n",
    "        logger.info(\"Collecting {} blocks for frequency calculation\", len(coordinates_to_track))\n",
    "        for coords in tqdm(coordinates_to_track):\n",
    "            numerical_id = self._get_block(coords[0], coords[1], coords[2])\n",
    "            # treating all meta of same id the same for simplicity\n",
    "                \n",
    "            item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                \n",
    "            self.block_frequency[item_id] += 1\n",
    "\n",
    "        logger.info(f\"Found {len(self.block_frequency)} unique blocks\")\n",
    "        \n",
    "        self.block2idx = dict()\n",
    "        self.idx2block = dict()\n",
    "        for name, count in self.block_frequency.items():\n",
    "            block_idx = len(self.block2idx)\n",
    "            self.block2idx[name] = block_idx\n",
    "            self.idx2block[block_idx] = name\n",
    "        logger.info(\"idx2block and block2idx dictionaries generated\")\n",
    "        \n",
    "    # return the block id at x y z coords\n",
    "    def _get_block(self, x, y, z):\n",
    "        # returns the id for the block\n",
    "        return self.world[x][y][z]\n",
    "\n",
    "    # return a list of block ids of neighboring blocks\n",
    "    def _get_neighbors(self, x, y, z, neighbor_radius=1):\n",
    "        neighbor_coords = [(x + x_diff, y + y_diff, z + z_diff) for x_diff, y_diff, z_diff in product(list(range(-neighbor_radius, neighbor_radius + 1)), repeat=3) if x_diff != 0 or y_diff != 0 or z_diff != 0]\n",
    "        return [self._get_block(*coord) for coord in neighbor_coords]\n",
    "\n",
    "    # generate the full list of coords within limits\n",
    "    def _gen_coords(self, x_lims, y_lims, z_lims):\n",
    "        return [(x, y, z) for x, y, z in product(range(x_lims[0], x_lims[1] + 1), range(y_lims[0], y_lims[1] + 1), range(z_lims[0], z_lims[1] + 1))]\n",
    "        \n",
    "    # make target context pairs while ignoring all air places\n",
    "    def _preprpcess(self):\n",
    "        air_removed = 0\n",
    "        preprocessed_dataset = [] # list of [target, contex]\n",
    "        \n",
    "        x_lims, y_lims, z_lims = self._read_size(neighbor_radius=self.neighbor_radius)\n",
    "        coordinates_to_preprocess = self._gen_coords(*(self._read_size(neighbor_radius=self.neighbor_radius)))\n",
    "        logger.info(\"Preprocessing {} blocks for target context pairs creation\", len(coordinates_to_preprocess))\n",
    "        \n",
    "        for coords in tqdm(coordinates_to_preprocess):\n",
    "            numerical_id = self._get_block(coords[0], coords[1], coords[2])\n",
    "            neighbors_numerical_ids = self._get_neighbors(*coords)\n",
    "            \n",
    "            if numerical_id == 0 and neighbors_numerical_ids == [0]*len(neighbors_numerical_ids):\n",
    "                # remove and log if too much air\n",
    "                air_removed += 1\n",
    "            else:\n",
    "                # if not airy add to dataset\n",
    "                item_id = self.block_id_lookup_dict[str(numerical_id)]\n",
    "                target = self.block2idx[item_id]\n",
    "                # target = torch.tensor(int(target))\n",
    "                \n",
    "                neighbors_item_ids = [self.block_id_lookup_dict[str(numerical_id)] for numerical_id in neighbors_numerical_ids]\n",
    "                context = [self.block2idx[item_id] for item_id in neighbors_item_ids]\n",
    "                # context = torch.tensor(context)\n",
    "                \n",
    "                preprocessed_dataset.append((target, context))\n",
    "        \n",
    "        # turn into np array\n",
    "        self.preprocessed_dataset = np.array(preprocessed_dataset)\n",
    "                \n",
    "        logger.info(f\"Removed {air_removed} examples that are too airy.\")\n",
    "        logger.info(f\"Generated {self.__len__()} example data preprocessed dataset.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.preprocessed_dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target, context = self.preprocessed_dataset[index]\n",
    "        return torch.tensor(target), torch.tensor(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block2Vec(pl.LightningModule):\n",
    "    def __init__(self, embedding_dim = 32, initial_lr = 1e-3, neighbor_radius = 1, batch_size = 256, num_epochs = 30, preprocess = True, save_preprocessed = True, load_preprocessed = False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # making lightning save params under self.hparams\n",
    "        \n",
    "        self.dataset = Block2VecDatasetV2(neighbor_radius=neighbor_radius, preprocess=preprocess, save_preprocessed=save_preprocessed, load_preprocessed=load_preprocessed)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = initial_lr # initial learning rate\n",
    "        self.neighbor_radius = neighbor_radius\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.output_path = \"output/block2vec\"\n",
    "\n",
    "        self.num_embeddings = len(self.dataset.block2idx)\n",
    "        self.model = SkipGramModel(self.num_embeddings, self.embedding_dim)\n",
    "        self.textures = dict()\n",
    "        \n",
    "        # print(self.model)\n",
    "        \n",
    "    def forward(self, target, context) -> torch.Tensor:\n",
    "        return self.model(target, context)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.forward(*batch)\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            math.ceil(len(self.dataset) / self.batch_size) *\n",
    "            self.num_epochs,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        embedding_dict = self.save_embedding(self.dataset.idx2block, self.output_path)\n",
    "        embedding_dict = self.save_embedding(\n",
    "            self.dataset.idx2block, self.output_path\n",
    "        )\n",
    "        self.create_confusion_matrix(\n",
    "            self.dataset.idx2block, self.output_path)\n",
    "        # self.plot_embeddings(embedding_dict, self.output_path)\n",
    "\n",
    "    def read_texture(self, block: str):\n",
    "        if block not in self.textures and block != \"air\":\n",
    "            texture_candidates = Path(\n",
    "                \"/home/schubert/projects/TOAD-GAN/minecraft/block2vec/textures\"\n",
    "            ).glob(\"*.png\")\n",
    "            # use of absolute path is intentional\n",
    "            match = process.extractOne(block, texture_candidates)\n",
    "            if match is not None:\n",
    "                logger.info(\"Matches {} with {} texture file\", block, match[0])\n",
    "                self.textures[block] = plt.imread(match[0])\n",
    "        if block not in self.textures:\n",
    "            self.textures[block] = np.ones(shape=[16, 16, 3])\n",
    "        return self.textures[block]\n",
    "\n",
    "    def save_embedding(self, id2block: Dict[int, str], output_path: str):\n",
    "        embeddings = self.model.target_embeddings.weight\n",
    "        # embeddings = embeddings / torch.norm(embeddings, p=2, dim=-1, keepdim=True)\n",
    "        embeddings = embeddings.cpu().data.numpy()\n",
    "        embedding_dict = {}\n",
    "        with open(os.path.join(output_path, \"embeddings.txt\"), \"w\") as f:\n",
    "            f.write(\"%d %d\\n\" % (len(id2block), self.embedding_dim))\n",
    "            for wid, w in id2block.items():\n",
    "                e = \" \".join(map(lambda x: str(x), embeddings[wid]))\n",
    "                embedding_dict[w] = torch.from_numpy(embeddings[wid])\n",
    "                f.write(\"%s %s\\n\" % (w, e))\n",
    "        np.save(os.path.join(output_path, \"embeddings.npy\"), embeddings)\n",
    "        with open(os.path.join(output_path, f\"representations.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(embedding_dict, f)\n",
    "        return embedding_dict\n",
    "    \n",
    "    def plot_embeddings(self, embedding_dict: Dict[str, np.ndarray], output_path: str):\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        legend = [label.replace(\"minecraft:\", \"\")\n",
    "                  for label in embedding_dict.keys()]\n",
    "        texture_imgs = [self.read_texture(block) for block in legend]\n",
    "        embeddings = torch.stack(list(embedding_dict.values())).numpy()\n",
    "        if embeddings.shape[-1] != 3:\n",
    "            embeddings_3d = umap.UMAP(\n",
    "                n_neighbors=5, min_dist=0.3, n_components=3\n",
    "            ).fit_transform(embeddings)\n",
    "        else:\n",
    "            embeddings_3d = embeddings\n",
    "        for embedding in embeddings_3d:\n",
    "            ax.scatter(*embedding, alpha=0)\n",
    "        ia = ImageAnnotations3D(embeddings_3d, texture_imgs, legend, ax, fig)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, \"scatter_3d.png\"), dpi=300)\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    def create_confusion_matrix(self, id2block: Dict[int, str], output_path: str):\n",
    "        rcParams.update({\"font.size\": 6})\n",
    "        names = []\n",
    "        dists = np.zeros((len(id2block), len(id2block)))\n",
    "        for i, b1 in id2block.items():\n",
    "            names.append(b1.split(\":\")[1])\n",
    "            for j, b2 in id2block.items():\n",
    "                dists[i, j] = F.mse_loss(\n",
    "                    self.model.target_embeddings.weight.data[i],\n",
    "                    self.model.target_embeddings.weight.data[j],\n",
    "                )\n",
    "        confusion_display = ConfusionMatrixDisplay(dists, display_labels=names)\n",
    "        confusion_display.plot(include_values=False,\n",
    "                               xticks_rotation=\"vertical\")\n",
    "        confusion_display.ax_.set_xlabel(\"\")\n",
    "        confusion_display.ax_.set_ylabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_path, \"dist_matrix.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-02-14T20:58:57.228924+0800\u001b[0m \u001b[31m\u001b[1mPreprocessing block2vec dataset\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa85b352c69e4638b6aeba63c9c1d1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1977 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1977 houses\n",
      "\u001b[34m2022-02-14T20:59:01.620891+0800\u001b[0m \u001b[31m\u001b[1mLoaded in world with shape: (63264, 32, 32)\u001b[0m\n",
      "\u001b[34m2022-02-14T20:59:09.415311+0800\u001b[0m \u001b[31m\u001b[1mCollecting 64782336 blocks for frequency calculation\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29880963ea0d41dcb64884bdb226b5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64782336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-02-14T21:00:40.612596+0800\u001b[0m \u001b[31m\u001b[1mFound 218 unique blocks\u001b[0m\n",
      "\u001b[34m2022-02-14T21:00:40.614038+0800\u001b[0m \u001b[31m\u001b[1midx2block and block2idx dictionaries generated\u001b[0m\n",
      "\u001b[34m2022-02-14T21:00:49.439889+0800\u001b[0m \u001b[31m\u001b[1mPreprocessing 56935800 blocks for target context pairs creation\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865d5509fc149349a4e18c698d1cdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56935800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 15091/15091 [1:26:56<00:00,  2.89it/s, loss=1.52, v_num=1]\n",
      "Epoch 0:   0%|          | 0/18 [30:45<?, ?it/s]\n",
      "Epoch 15:  56%|█████▌    | 10/18 [29:59<23:59, 179.94s/it, loss=0.915, v_num=4]\n",
      "Epoch 10: 100%|██████████| 18/18 [28:00<00:00, 93.39s/it, loss=0.926, v_num=5]\n",
      "\u001b[34m2022-02-14T21:48:35.450607+0800\u001b[0m \u001b[31m\u001b[1mRemoved 53072752 examples that are too airy.\u001b[0m\n",
      "\u001b[34m2022-02-14T21:48:35.452199+0800\u001b[0m \u001b[31m\u001b[1mGenerated 3863048 example data preprocessed dataset.\u001b[0m\n",
      "\u001b[34m2022-02-14T21:48:35.560391+0800\u001b[0m \u001b[31m\u001b[1mSaving preprocessed block2vec dataset to file\u001b[0m\n",
      "\u001b[34m2022-02-14T21:48:39.056483+0800\u001b[0m \u001b[31m\u001b[1mSaving idx2block and block2idx\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | SkipGramModel | 28.1 K\n",
      "----------------------------------------\n",
      "28.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.1 K    Total params\n",
      "0.112     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 15091/15091 [02:42<00:00, 92.68it/s, loss=1.5, v_num=7]  \n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 32\n",
    "\n",
    "block2vec = Block2Vec(embedding_dim = 64, initial_lr = 2e-3, neighbor_radius = 1, batch_size = 256, num_epochs = NUM_EPOCHS, preprocess = False, save_preprocessed = False, load_preprocessed = True)\n",
    "trainer = pl.Trainer(gpus=0, max_epochs=NUM_EPOCHS, fast_dev_run=False)\n",
    "trainer.fit(block2vec)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb97c89c7d33bf00e68473e7beaa192cc5ff3ebaf4fd59b3c63e5a73e8b6a1d4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('world-gan-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
